{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO\n",
    "\n",
    "This notebook covers various optimization problems related to the LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.loadtxt(\"X.csv\", delimiter=',')\n",
    "Y = np.loadtxt(\"Y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given $X, Y$, here is the squared error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\frac{C}{2}\\left\\|X_{}\\beta - Y_{}\\right\\|^2_2$$"
      ],
      "text/plain": [
       "affine_smooth(quadratic((442,), coef=1, Q=None, Qdiag=False, offset=[ 151.   75.  141.  206.  135.   97.  138.   63.  110.  310.  101.   69.\n",
       "  179.  185.  118.  171.  166.  144.   97.  168.   68.   49.   68.  245.\n",
       "  184.  202.  137.   85.  131.  283.  129.   59.  341.   87.   65.  102.\n",
       "  265.  276.  252.   90.  100.   55.   61.   92.  259.   53.  190.  142.\n",
       "   75.  142.  155.  225.   59.  104.  182.  128.   52.   37.  170.  170.\n",
       "   61.  144.   52.  128.   71.  163.  150.   97.  160.  178.   48.  270.\n",
       "  202.  111.   85.   42.  170.  200.  252.  113.  143.   51.   52.  210.\n",
       "   65.  141.   55.  134.   42.  111.   98.  164.   48.   96.   90.  162.\n",
       "  150.  279.   92.   83.  128.  102.  302.  198.   95.   53.  134.  144.\n",
       "  232.   81.  104.   59.  246.  297.  258.  229.  275.  281.  179.  200.\n",
       "  200.  173.  180.   84.  121.  161.   99.  109.  115.  268.  274.  158.\n",
       "  107.   83.  103.  272.   85.  280.  336.  281.  118.  317.  235.   60.\n",
       "  174.  259.  178.  128.   96.  126.  288.   88.  292.   71.  197.  186.\n",
       "   25.   84.   96.  195.   53.  217.  172.  131.  214.   59.   70.  220.\n",
       "  268.  152.   47.   74.  295.  101.  151.  127.  237.  225.   81.  151.\n",
       "  107.   64.  138.  185.  265.  101.  137.  143.  141.   79.  292.  178.\n",
       "   91.  116.   86.  122.   72.  129.  142.   90.  158.   39.  196.  222.\n",
       "  277.   99.  196.  202.  155.   77.  191.   70.   73.   49.   65.  263.\n",
       "  248.  296.  214.  185.   78.   93.  252.  150.   77.  208.   77.  108.\n",
       "  160.   53.  220.  154.  259.   90.  246.  124.   67.   72.  257.  262.\n",
       "  275.  177.   71.   47.  187.  125.   78.   51.  258.  215.  303.  243.\n",
       "   91.  150.  310.  153.  346.   63.   89.   50.   39.  103.  308.  116.\n",
       "  145.   74.   45.  115.  264.   87.  202.  127.  182.  241.   66.   94.\n",
       "  283.   64.  102.  200.  265.   94.  230.  181.  156.  233.   60.  219.\n",
       "   80.   68.  332.  248.   84.  200.   55.   85.   89.   31.  129.   83.\n",
       "  275.   65.  198.  236.  253.  124.   44.  172.  114.  142.  109.  180.\n",
       "  144.  163.  147.   97.  220.  190.  109.  191.  122.  230.  242.  248.\n",
       "  249.  192.  131.  237.   78.  135.  244.  199.  270.  164.   72.   96.\n",
       "  306.   91.  214.   95.  216.  263.  178.  113.  200.  139.  139.   88.\n",
       "  148.   88.  243.   71.   77.  109.  272.   60.   54.  221.   90.  311.\n",
       "  281.  182.  321.   58.  262.  206.  233.  242.  123.  167.   63.  197.\n",
       "   71.  168.  140.  217.  121.  235.  245.   40.   52.  104.  132.   88.\n",
       "   69.  219.   72.  201.  110.   51.  277.   63.  118.   69.  273.  258.\n",
       "   43.  198.  242.  232.  175.   93.  168.  275.  293.  281.   72.  140.\n",
       "  189.  181.  209.  136.  261.  113.  131.  174.  257.   55.   84.   42.\n",
       "  146.  212.  233.   91.  111.  152.  120.   67.  310.   94.  183.   66.\n",
       "  173.   72.   49.   64.   48.  178.  104.  132.  220.   57.]), <regreg.affine.linear_transform object at 0x103f87110>, store_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regreg.api as rr\n",
    "loss = rr.squared_error(X, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `loss` is an instance of `regreg.smooth.affine_smooth` the representation of a smooth function in `regreg` composed with a linear transformation. Its \n",
    "most important API piece is `smooth_objective` which evaluates the function, its gradient or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6425460.4999999991"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, score_at_zero = loss.smooth_objective(np.zeros(loss.shape), 'both')\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-304.18307453,  -69.71535568, -949.43526038, -714.7416437 ,\n",
       "        -343.25445189, -281.78459335,  639.14527932, -696.88303009,\n",
       "        -916.13872282, -619.22282068]),\n",
       " array([-304.18307453,  -69.71535568, -949.43526038, -714.7416437 ,\n",
       "        -343.25445189, -281.78459335,  639.14527932, -696.88303009,\n",
       "        -916.13872282, -619.22282068]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_at_zero, X.T.dot(X.dot(np.zeros(loss.shape)) - Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LASSO uses an $\\ell_1$ penalty in \"Lagrange\" form:\n",
    "$$\n",
    "\\text{minimize}_{\\beta} \\frac{1}{2} \\|Y-X\\beta\\|^2_2 + \\lambda \\|\\beta\\|_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('penalty:', 'l1norm((10,), lagrange=200.000000, offset=None)')\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\lambda_{} \\|\\beta\\|_1$$"
      ],
      "text/plain": [
       "l1norm((10,), lagrange=200.000000, offset=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty = rr.l1norm(10, lagrange=200.)\n",
    "print ('penalty:', str(penalty))\n",
    "penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object penalty is an instance of `regreg.atoms.seminorm`. The main API used in `regreg`\n",
    "is the method `proximal` which computes the proximal mapping of the object. In `regreg`, an `atom` generally means it has a simple proximal map.\n",
    "\n",
    "The proximal mapping of the function \n",
    "$$\n",
    "f(\\beta) = \\lambda \\|\\beta\\|_1\n",
    "$$\n",
    "is\n",
    "$$\n",
    "\\text{prox}_{f, \\epsilon}(z) = \\text{argmin}_{\\beta} \\left[\\frac{\\epsilon}{2}\\|\\beta-z\\|^2_2 + f(\\beta)\\right].\n",
    "$$\n",
    "\n",
    "See [this document](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf) for a brief review of proximal maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $f$ is as above, this is the soft-thresholding map\n",
    "$$\n",
    "\\text{prox}_{f,\\epsilon}(z)_i = \n",
    "\\begin{cases}\n",
    "\\text{sign}(z_i)(|z_i| - \\lambda / \\epsilon) & |z_i| > \\lambda  / \\epsilon \\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "More generally, we might want to solve\n",
    "$$\n",
    "\\text{minimize}_{\\beta} \\left[\\frac{C}{2} \\|\\beta-\\mu\\|^2_2 + \\eta^T\\beta + \\gamma + f(\\beta)\\right]\n",
    "$$\n",
    "which can easily done if we know the proximal mapping.\n",
    "\n",
    "In `regreg`, objects $Q$ of the form\n",
    "$$\n",
    "Q(\\beta) =  \\frac{C}{2} \\|\\beta-\\mu\\|^2_2 + \\eta^T\\beta + \\gamma\n",
    "$$\n",
    "are represented instances of `rr.identity_quadratic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.random.standard_normal(penalty.shape)\n",
    "penalty.lagrange = 0.1\n",
    "epsilon = 0.4\n",
    "quadratic_term = rr.identity_quadratic(epsilon, Z, 0, 0)\n",
    "penalty.proximal(quadratic_term) - penalty.solve(quadratic_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.51405235,  0.15015721,  0.72873798,  1.9908932 ,  1.61755799,\n",
       "       -0.72727788,  0.70008842,  0.        ,  0.        ,  0.1605985 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = penalty.lagrange / epsilon\n",
    "soft_thresh_Z = np.sign(Z) * (np.fabs(Z) - threshold) * (np.fabs(Z) > threshold)\n",
    "soft_thresh_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects `loss` and `penalty` are combined to form the LASSO objective above. \n",
    "This is the canonical problem that we want to solve:\n",
    "$$\n",
    "\\text{minimize}_{\\beta} f(\\beta) + g(\\beta)\n",
    "$$\n",
    "where $f$ is a smooth convex function (i.e. we can compute its value and its gradient)\n",
    "and $g$ is a function whose proximal map is easy to compute.\n",
    "\n",
    "The object `rr.simple_problem` requires its first argument to have a `smooth_objective`\n",
    "method and its second argument to have a `solve` method that solves\n",
    "$$\n",
    "\\text{minimize}_{\\beta} g(\\beta) + Q(\\beta)\n",
    "$$\n",
    "where $Q$ is a quadratic of the above form. If $g$ has a `proximal` method, this step\n",
    "just calls the proximal mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\begin{aligned}\n",
       "\\text{minimize}_{\\beta} & f(\\beta) + g(\\beta) \\\\\n",
       "f(\\beta) &= \\frac{C}{2}\\left\\|X_{1}\\beta - Y_{1}\\right\\|^2_2 \\\\\n",
       "g(\\beta) &= \\lambda_{2} \\|\\beta\\|_1 \\\\\n",
       "\\end{aligned}\n",
       "$$"
      ],
      "text/plain": [
       "<regreg.problems.simple.simple_problem at 0x10527dc90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty.lagrange = 200.\n",
    "problem_lagrange = rr.simple_problem(loss, penalty)\n",
    "problem_lagrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.           -0.          479.01574743  149.17368192   -0.           -0.\n",
      "  -71.22719502    0.          415.33632118    0.        ]\n"
     ]
    }
   ],
   "source": [
    "coef_lagrange = problem_lagrange.solve(tol=1.e-12)\n",
    "print(coef_lagrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114.75294555\n"
     ]
    }
   ],
   "source": [
    "implied_bound = np.fabs(coef_lagrange).sum()\n",
    "print(implied_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$I^{\\infty}(\\|\\beta\\|_1 \\leq \\delta_{})$$"
      ],
      "text/plain": [
       "l1norm((10,), bound=1114.752946, offset=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bound_constraint = rr.l1norm(10, bound=implied_bound)\n",
    "bound_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\begin{aligned}\n",
       "\\text{minimize}_{\\beta} & f(\\beta) + g(\\beta) \\\\\n",
       "f(\\beta) &= \\frac{C}{2}\\left\\|X_{1}\\beta - Y_{1}\\right\\|^2_2 \\\\\n",
       "g(\\beta) &= I^{\\infty}(\\|\\beta\\|_1 \\leq \\delta_{2}) \\\\\n",
       "\\end{aligned}\n",
       "$$"
      ],
      "text/plain": [
       "<regreg.problems.simple.simple_problem at 0x10527dd90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_bound = rr.simple_problem(loss, bound_constraint)\n",
    "problem_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -0.            0.          479.01516494  149.1742413     0.            0.\n",
      "  -71.22753915   -0.          415.33600015   -0.        ]\n"
     ]
    }
   ],
   "source": [
    "coef_bound = problem_bound.solve(tol=1.e-12)\n",
    "print(coef_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4266132415834472e-06"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(coef_bound - coef_lagrange) / np.linalg.norm(coef_lagrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to `sklearn`\n",
    "\n",
    "The objective function is differs from `sklearn.linear_model.Lasso` by a factor of $1/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,   -0.        ,  479.04638168,  149.15882396,\n",
       "         -0.        ,   -0.        ,  -71.20460087,    0.        ,\n",
       "        415.33678368,    0.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "clf = Lasso(alpha=penalty.lagrange / X.shape[0])\n",
    "sklearn_soln = clf.fit(X, Y).coef_\n",
    "sklearn_soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtiming = np.random.standard_normal((2000, 4000))\n",
    "Ytiming = np.random.standard_normal(2000)\n",
    "lagrange = np.fabs(Xtiming.T.dot(Ytiming)).max() * 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 455 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "clf = Lasso(alpha=lagrange / Xtiming.shape[0])\n",
    "sklearn_soln = clf.fit(Xtiming, Ytiming).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 663 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "loss = rr.squared_error(Xtiming, Ytiming)\n",
    "penalty = rr.l1norm(Xtiming.shape[1], lagrange=lagrange)\n",
    "rr.simple_problem(loss,penalty).solve(tol=1.e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1031.243283031072, 1031.2492548036926)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_t = rr.squared_error(Xtiming, Ytiming)\n",
    "penalty_t = rr.l1norm(Xtiming.shape[1], lagrange=lagrange)\n",
    "soln1 = rr.simple_problem(loss_t, penalty_t).solve(tol=1.e-6)\n",
    "clf = Lasso(alpha=lagrange / Xtiming.shape[0])\n",
    "soln2 = clf.fit(Xtiming, Ytiming).coef_\n",
    "print (soln1 != 0).sum(), (soln2 != 0).sum()\n",
    "np.linalg.norm(soln1 - soln2) / np.linalg.norm(soln1)\n",
    "(loss_t.smooth_objective(soln1, 'func') + np.fabs(soln1).sum() * lagrange, loss_t.smooth_objective(soln2, 'func') + np.fabs(soln2).sum() * lagrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,   -0.        ,  479.04638168,  149.15882396,\n",
       "         -0.        ,   -0.        ,  -71.20460087,    0.        ,\n",
       "        415.33678368,    0.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.2369887653135822e-05"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(sklearn_soln - coef_lagrange) / np.linalg.norm(coef_lagrange)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net\n",
    "\n",
    "The elastic net differs from the LASSO only by addition of a quadratic term.\n",
    "In `regreg`, both smooth functions and atoms have their own quadratic term that\n",
    "is added to the objective before solving the problem. \n",
    "\n",
    "The `identity_quadratic` is specified as $Q$ above:\n",
    "$$\n",
    "Q(\\beta) = \\frac{C}{2} \\|\\beta-\\mu\\|^2_2 + \\eta^T\\beta + \\gamma\n",
    "$$\n",
    "with $C$ the first argument, $\\mu$ the second, $\\eta$ the third and $\\gamma$ the fourth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{equation*} \\frac{L_{}}{2}\\|\\beta\\|^2_2 \\end{equation*} "
      ],
      "text/plain": [
       "identity_quadratic(0.500000, 0.0, 0.0, 0.000000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet_term = rr.identity_quadratic(0.5,0,0,0)\n",
    "enet_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\lambda_{} \\|\\beta\\|_1 + \\frac{L_{}}{2}\\|\\beta\\|^2_2$$"
      ],
      "text/plain": [
       "l1norm((10,), lagrange=200.000000, offset=None, quadratic=identity_quadratic(0.500000, 0.0, 0.0, 0.000000))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty_enet = rr.l1norm(10, lagrange=200., quadratic=enet_term)\n",
    "penalty_enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,   -0.        ,  330.99470749,  153.30113271,\n",
       "          0.        ,    0.        ,  -90.63802712,   40.66226508,\n",
       "        286.38998267,   37.24626444])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_enet = rr.simple_problem(loss, penalty_enet)\n",
    "enet_lagrange = problem_enet.solve(min_its=200, tol=1.e-12)\n",
    "enet_lagrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic terms can also be added to problems as the first argument to `solve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,   -0.        ,  330.99470749,  153.30113271,\n",
       "          0.        ,    0.        ,  -90.63802712,   40.66226508,\n",
       "        286.38998267,   37.24626444])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_lagrange.solve(enet_term, min_its=200, tol=1.e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Objects like `enet_term` are ubiquitous in `regreg` because it is a package\n",
    "that uses proximal gradient methods to solve problems. Hence, it is repeatedly solving problems like\n",
    "$$\n",
    "\\text{minimize}_{\\beta} \\frac{C}{2} \\|z-\\beta\\|^2_2 + {\\cal P}(\\beta).\n",
    "$$\n",
    "\n",
    "It therefore manipulates these objects in the course of solving the problem.\n",
    "The arguments to `rr.identity_quadratic` determine functions like\n",
    "$$\n",
    "\\beta \\mapsto \\frac{C}{2} \\|\\beta - \\mu\\|^2_2 + \\beta^T\\eta + \\gamma.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'identity_quadratic(0.500000, array([0, 1, 2, 3]), array([ 1.,  1.,  1.,  1.]), 2.300000)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 0.5 \n",
    "mu = np.arange(4)\n",
    "eta = np.ones(4)\n",
    "gamma = 2.3\n",
    "\n",
    "iq = rr.identity_quadratic(C, mu, eta, gamma)\n",
    "str(iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.7999999999999998, 5.7999999999999998)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = -np.ones(4)\n",
    "iq.objective(beta, 'func'), 0.5*C*((beta-mu)**2).sum() + (beta*eta).sum() + gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments $\\mu$ is the `center` and $\\eta$ is the `linear_term`, the argument $\\gamma$ is `constant` which seems somewhat unnecessary but is sometimes useful to track through computations.\n",
    "such that `center` is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'identity_quadratic(0.500000, 0.0, array([ 1. ,  0.5,  0. , -0.5]), 5.800000)'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(iq.collapsed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As atoms and smooth functions have their own such quadratic terms, one sometimes collects\n",
    "them to form an overall quadratic term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{equation*} \\frac{L_{}}{2}\\|\\beta-\\mu_{}\\|^2_2 + \\left \\langle \\eta_{}, \\beta \\right \\rangle + \\gamma_{}  \\end{equation*} "
      ],
      "text/plain": [
       "identity_quadratic(0.300000, array([ 1.,  1.,  1.,  1.]), array([0, 1, 2, 3]), -2.100000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iq2 = rr.identity_quadratic(0.3, eta, mu, -2.1)\n",
    "iq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'identity_quadratic(0.800000, 0.0, array([ 0.7,  1.2,  1.7,  2.2]), 4.300000)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(iq+iq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{equation*} \\frac{L_{}}{2}\\|\\beta\\|^2_2 + \\left \\langle \\eta_{}, \\beta \\right \\rangle + \\gamma_{}  \\end{equation*} "
      ],
      "text/plain": [
       "identity_quadratic(0.500000, 0.0, array([ 1. ,  0.5,  0. , -0.5]), 5.800000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iq.collapsed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual problems\n",
    "\n",
    "The LASSO or Elastic Net can often be solved by solving an associated dual problem.\n",
    "There are various ways to construct such problems. \n",
    "\n",
    "One such way is to write our elastic net problem as\n",
    "$$\n",
    "\\text{minimize}_{\\beta} f(\\beta) + g(\\beta)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\beta) &= \\frac{1}{2} \\|Y-X\\beta\\|^2_2 + \\frac{C}{2} \\|\\beta\\|^2_2 \\\\\n",
    "g(\\beta) &= \\lambda \\|\\beta\\|_1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then, we duplicate the variable $\\beta$ yielding\n",
    "$$\n",
    "\\text{minimize}_{\\beta_1,\\beta_2:\\beta_1=\\beta_2} f(\\beta_1) + g(\\beta_2)\n",
    "$$\n",
    "and introduce the Lagrangian\n",
    "$$\n",
    "L(\\beta_1,\\beta_2,u) = f(\\beta_1) + g(\\beta_2) + u^T(\\beta_1-\\beta_2).\n",
    "$$\n",
    "\n",
    "The dual problem is constructed by minimizing over $(\\beta_1,\\beta_2)$ which yields a function of\n",
    "$u$:\n",
    "$$\n",
    "\\inf_{\\beta_1,\\beta_2}L(\\beta_1,\\beta_2,u) = -f^*(-u) - g^*(u)\n",
    "$$\n",
    "where \n",
    "$$\n",
    "f^*(u) = \\sup_{\\beta} \\beta^Tu - f(\\beta)\n",
    "$$\n",
    "is the convex conjugate of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual problem, written as a minimization problem is\n",
    "$$\n",
    "\\text{minimize}_{u} f^*(-u) + g^*(u).\n",
    "$$\n",
    "\n",
    "In the elastic net case, \n",
    "$$\n",
    "g^*(u) = I^{\\infty}(\\|u\\|_{\\infty} \\leq \\lambda)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^*(-u) &= -\\inf_{\\beta}\\left[ \\frac{1}{2} \\|Y-X\\beta\\|^2_2 + \\frac{C}{2}\\|\\beta\\|^2_2 + u^T\\beta\\right] \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the optimal $\\beta$ in computing the infimum aboves satisfies the normal equations\n",
    "$$\n",
    "(X^TX + C \\cdot I)\\beta^*(u,Y) = X^TY - u\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\beta^*(u,Y) = (X^TX+C \\cdot I)^{-1}(X^TY-u).\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "f^*(-u) = \\frac{1}{2} (X^TY-u)^T(X^TX+C \\cdot I)^{-1}(X^TY-u) - \\frac{1}{2}\\|Y\\|^2_2.\n",
    "$$\n",
    "\n",
    "The function $f^*$ can be evaluated exactly as it is quadratic, though it can also be solved numerically if \n",
    "our loss was not squared-error. This is what the class `regreg.api.conjugate` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02091801882e-05\n",
      "-5883988.88044 -5883988.88044\n"
     ]
    }
   ],
   "source": [
    "dual_loss = rr.conjugate(loss, negate=True, quadratic=enet_term, tol=1.e-12)\n",
    "Q = np.linalg.inv(X.T.dot(X) + enet_term.coef * np.identity(10))\n",
    "\n",
    "def dual_loss_explicit(u):\n",
    "    z = X.T.dot(Y) - u\n",
    "    return 0.5 * (z * Q.dot(z)).sum() - 0.5 * (Y**2).sum()\n",
    "\n",
    "U = np.random.standard_normal(10) * 1\n",
    "print np.linalg.norm((dual_loss.smooth_objective(U, 'grad') + Q.dot(X.T.dot(Y) - U)))  / np.linalg.norm(dual_loss.smooth_objective(U, 'grad'))\n",
    "print dual_loss.smooth_objective(U, 'func'), dual_loss_explicit(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `negate` option tells `regreg` that the function we want is the conjugate of `loss` composed with\n",
    "a sign change, i.e. a linear transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supnorm((10,), bound=200.000000, offset=None)\n"
     ]
    }
   ],
   "source": [
    "dual_atom = penalty.conjugate\n",
    "print str(dual_atom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  87.61073573,  -94.96475265,  200.        ,  200.        ,\n",
       "         46.2648192 ,   20.2595554 , -200.        ,  200.        ,\n",
       "        200.        ,  200.        ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dual_problem = rr.simple_problem(dual_loss, dual_atom)\n",
    "dual_soln = dual_problem.solve(min_its=50)\n",
    "dual_soln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this dual problem is equal to the negative of the gradient of the objective of our \n",
    "elastic net at the solution. This is sometimes referred to as a primal-dual relationship, and is\n",
    "in effect a restatement of the KKT conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  87.61073546,  -94.96475199,  200.        ,  200.        ,\n",
       "         46.26481699,   20.2595532 , -200.        ,  200.        ,\n",
       "        200.        ,  200.        ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- loss.smooth_objective(enet_lagrange, 'grad') - enet_term.objective(enet_lagrange, 'grad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `conjugate` object, `regreg` retains a reference to the minimizer, i.e. the gradient of the\n",
    "conjugate function. In our problem, this is actually the solution to our elastic net problem, though it\n",
    "does not have exact zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primal_soln = dual_loss.argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.16535318e-07,   4.82057016e-07,   3.30994708e+02,\n",
       "         1.53301133e+02,  -2.22893096e-06,  -2.18321317e-06,\n",
       "        -9.06380260e+01,   4.06622669e+01,   2.86389983e+02,\n",
       "         3.72462648e+01])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primal_soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.18565315273e-09\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.norm(primal_soln - enet_lagrange) / np.linalg.norm(enet_lagrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could alternatively have formed the explicit quadratic function for $f^*(-u)$. Having formed the \n",
    "quadratic objective explicitly, we will have to also explicitly solve for the primal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  87.61073546,  -94.96475199,  200.        ,  200.        ,\n",
       "         46.26481699,   20.2595532 , -200.        ,  200.        ,\n",
       "        200.        ,  200.        ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dual_quadratic = rr.quadratic(Q.shape[0], Q=Q, offset=X.T.dot(Y))\n",
    "dual_problem_alt = rr.simple_problem(dual_quadratic, dual_atom)\n",
    "dual_soln_alt = dual_problem_alt.solve(min_its=100)\n",
    "dual_soln_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.81472493994e-13\n"
     ]
    }
   ],
   "source": [
    "primal_soln_alt = -dual_quadratic.smooth_objective(dual_soln_alt, 'grad')\n",
    "print np.linalg.norm(primal_soln_alt - enet_lagrange) / np.linalg.norm(enet_lagrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis pursuit\n",
    "\n",
    "Yet another species in the zoology of LASSO problems is the basis pursuit problem\n",
    "$$\n",
    "\\text{minimize}_{\\beta: \\|y-X\\beta\\|_2 \\leq \\delta} \\|\\beta\\|_1.\n",
    "$$\n",
    "This can be written as the sum of two atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\lambda_{} \\|\\beta\\|_1$$"
      ],
      "text/plain": [
       "l1norm((10,), lagrange=1.000000, offset=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_part = rr.l1norm(X.shape[1], lagrange=1.)\n",
    "l1_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$I^{\\infty}(\\|X_{}\\beta - \\alpha_{}\\|_2 \\leq \\delta_{})$$"
      ],
      "text/plain": [
       "affine_atom(l2norm((442,), bound=1236.697060, offset=array([ -1.13348416e+00,  -7.71334842e+01,  -1.11334842e+01,\n",
       "         5.38665158e+01,  -1.71334842e+01,  -5.51334842e+01,\n",
       "        -1.41334842e+01,  -8.91334842e+01,  -4.21334842e+01,\n",
       "         1.57866516e+02,  -5.11334842e+01,  -8.31334842e+01,\n",
       "         2.68665158e+01,   3.28665158e+01,  -3.41334842e+01,\n",
       "         1.88665158e+01,   1.38665158e+01,  -8.13348416e+00,\n",
       "        -5.51334842e+01,   1.58665158e+01,  -8.41334842e+01,\n",
       "        -1.03133484e+02,  -8.41334842e+01,   9.28665158e+01,\n",
       "         3.18665158e+01,   4.98665158e+01,  -1.51334842e+01,\n",
       "        -6.71334842e+01,  -2.11334842e+01,   1.30866516e+02,\n",
       "        -2.31334842e+01,  -9.31334842e+01,   1.88866516e+02,\n",
       "        -6.51334842e+01,  -8.71334842e+01,  -5.01334842e+01,\n",
       "         1.12866516e+02,   1.23866516e+02,   9.98665158e+01,\n",
       "        -6.21334842e+01,  -5.21334842e+01,  -9.71334842e+01,\n",
       "        -9.11334842e+01,  -6.01334842e+01,   1.06866516e+02,\n",
       "        -9.91334842e+01,   3.78665158e+01,  -1.01334842e+01,\n",
       "        -7.71334842e+01,  -1.01334842e+01,   2.86651584e+00,\n",
       "         7.28665158e+01,  -9.31334842e+01,  -4.81334842e+01,\n",
       "         2.98665158e+01,  -2.41334842e+01,  -1.00133484e+02,\n",
       "        -1.15133484e+02,   1.78665158e+01,   1.78665158e+01,\n",
       "        -9.11334842e+01,  -8.13348416e+00,  -1.00133484e+02,\n",
       "        -2.41334842e+01,  -8.11334842e+01,   1.08665158e+01,\n",
       "        -2.13348416e+00,  -5.51334842e+01,   7.86651584e+00,\n",
       "         2.58665158e+01,  -1.04133484e+02,   1.17866516e+02,\n",
       "         4.98665158e+01,  -4.11334842e+01,  -6.71334842e+01,\n",
       "        -1.10133484e+02,   1.78665158e+01,   4.78665158e+01,\n",
       "         9.98665158e+01,  -3.91334842e+01,  -9.13348416e+00,\n",
       "        -1.01133484e+02,  -1.00133484e+02,   5.78665158e+01,\n",
       "        -8.71334842e+01,  -1.11334842e+01,  -9.71334842e+01,\n",
       "        -1.81334842e+01,  -1.10133484e+02,  -4.11334842e+01,\n",
       "        -5.41334842e+01,   1.18665158e+01,  -1.04133484e+02,\n",
       "        -5.61334842e+01,  -6.21334842e+01,   9.86651584e+00,\n",
       "        -2.13348416e+00,   1.26866516e+02,  -6.01334842e+01,\n",
       "        -6.91334842e+01,  -2.41334842e+01,  -5.01334842e+01,\n",
       "         1.49866516e+02,   4.58665158e+01,  -5.71334842e+01,\n",
       "        -9.91334842e+01,  -1.81334842e+01,  -8.13348416e+00,\n",
       "         7.98665158e+01,  -7.11334842e+01,  -4.81334842e+01,\n",
       "        -9.31334842e+01,   9.38665158e+01,   1.44866516e+02,\n",
       "         1.05866516e+02,   7.68665158e+01,   1.22866516e+02,\n",
       "         1.28866516e+02,   2.68665158e+01,   4.78665158e+01,\n",
       "         4.78665158e+01,   2.08665158e+01,   2.78665158e+01,\n",
       "        -6.81334842e+01,  -3.11334842e+01,   8.86651584e+00,\n",
       "        -5.31334842e+01,  -4.31334842e+01,  -3.71334842e+01,\n",
       "         1.15866516e+02,   1.21866516e+02,   5.86651584e+00,\n",
       "        -4.51334842e+01,  -6.91334842e+01,  -4.91334842e+01,\n",
       "         1.19866516e+02,  -6.71334842e+01,   1.27866516e+02,\n",
       "         1.83866516e+02,   1.28866516e+02,  -3.41334842e+01,\n",
       "         1.64866516e+02,   8.28665158e+01,  -9.21334842e+01,\n",
       "         2.18665158e+01,   1.06866516e+02,   2.58665158e+01,\n",
       "        -2.41334842e+01,  -5.61334842e+01,  -2.61334842e+01,\n",
       "         1.35866516e+02,  -6.41334842e+01,   1.39866516e+02,\n",
       "        -8.11334842e+01,   4.48665158e+01,   3.38665158e+01,\n",
       "        -1.27133484e+02,  -6.81334842e+01,  -5.61334842e+01,\n",
       "         4.28665158e+01,  -9.91334842e+01,   6.48665158e+01,\n",
       "         1.98665158e+01,  -2.11334842e+01,   6.18665158e+01,\n",
       "        -9.31334842e+01,  -8.21334842e+01,   6.78665158e+01,\n",
       "         1.15866516e+02,  -1.33484163e-01,  -1.05133484e+02,\n",
       "        -7.81334842e+01,   1.42866516e+02,  -5.11334842e+01,\n",
       "        -1.13348416e+00,  -2.51334842e+01,   8.48665158e+01,\n",
       "         7.28665158e+01,  -7.11334842e+01,  -1.13348416e+00,\n",
       "        -4.51334842e+01,  -8.81334842e+01,  -1.41334842e+01,\n",
       "         3.28665158e+01,   1.12866516e+02,  -5.11334842e+01,\n",
       "        -1.51334842e+01,  -9.13348416e+00,  -1.11334842e+01,\n",
       "        -7.31334842e+01,   1.39866516e+02,   2.58665158e+01,\n",
       "        -6.11334842e+01,  -3.61334842e+01,  -6.61334842e+01,\n",
       "        -3.01334842e+01,  -8.01334842e+01,  -2.31334842e+01,\n",
       "        -1.01334842e+01,  -6.21334842e+01,   5.86651584e+00,\n",
       "        -1.13133484e+02,   4.38665158e+01,   6.98665158e+01,\n",
       "         1.24866516e+02,  -5.31334842e+01,   4.38665158e+01,\n",
       "         4.98665158e+01,   2.86651584e+00,  -7.51334842e+01,\n",
       "         3.88665158e+01,  -8.21334842e+01,  -7.91334842e+01,\n",
       "        -1.03133484e+02,  -8.71334842e+01,   1.10866516e+02,\n",
       "         9.58665158e+01,   1.43866516e+02,   6.18665158e+01,\n",
       "         3.28665158e+01,  -7.41334842e+01,  -5.91334842e+01,\n",
       "         9.98665158e+01,  -2.13348416e+00,  -7.51334842e+01,\n",
       "         5.58665158e+01,  -7.51334842e+01,  -4.41334842e+01,\n",
       "         7.86651584e+00,  -9.91334842e+01,   6.78665158e+01,\n",
       "         1.86651584e+00,   1.06866516e+02,  -6.21334842e+01,\n",
       "         9.38665158e+01,  -2.81334842e+01,  -8.51334842e+01,\n",
       "        -8.01334842e+01,   1.04866516e+02,   1.09866516e+02,\n",
       "         1.22866516e+02,   2.48665158e+01,  -8.11334842e+01,\n",
       "        -1.05133484e+02,   3.48665158e+01,  -2.71334842e+01,\n",
       "        -7.41334842e+01,  -1.01133484e+02,   1.05866516e+02,\n",
       "         6.28665158e+01,   1.50866516e+02,   9.08665158e+01,\n",
       "        -6.11334842e+01,  -2.13348416e+00,   1.57866516e+02,\n",
       "         8.66515837e-01,   1.93866516e+02,  -8.91334842e+01,\n",
       "        -6.31334842e+01,  -1.02133484e+02,  -1.13133484e+02,\n",
       "        -4.91334842e+01,   1.55866516e+02,  -3.61334842e+01,\n",
       "        -7.13348416e+00,  -7.81334842e+01,  -1.07133484e+02,\n",
       "        -3.71334842e+01,   1.11866516e+02,  -6.51334842e+01,\n",
       "         4.98665158e+01,  -2.51334842e+01,   2.98665158e+01,\n",
       "         8.88665158e+01,  -8.61334842e+01,  -5.81334842e+01,\n",
       "         1.30866516e+02,  -8.81334842e+01,  -5.01334842e+01,\n",
       "         4.78665158e+01,   1.12866516e+02,  -5.81334842e+01,\n",
       "         7.78665158e+01,   2.88665158e+01,   3.86651584e+00,\n",
       "         8.08665158e+01,  -9.21334842e+01,   6.68665158e+01,\n",
       "        -7.21334842e+01,  -8.41334842e+01,   1.79866516e+02,\n",
       "         9.58665158e+01,  -6.81334842e+01,   4.78665158e+01,\n",
       "        -9.71334842e+01,  -6.71334842e+01,  -6.31334842e+01,\n",
       "        -1.21133484e+02,  -2.31334842e+01,  -6.91334842e+01,\n",
       "         1.22866516e+02,  -8.71334842e+01,   4.58665158e+01,\n",
       "         8.38665158e+01,   1.00866516e+02,  -2.81334842e+01,\n",
       "        -1.08133484e+02,   1.98665158e+01,  -3.81334842e+01,\n",
       "        -1.01334842e+01,  -4.31334842e+01,   2.78665158e+01,\n",
       "        -8.13348416e+00,   1.08665158e+01,  -5.13348416e+00,\n",
       "        -5.51334842e+01,   6.78665158e+01,   3.78665158e+01,\n",
       "        -4.31334842e+01,   3.88665158e+01,  -3.01334842e+01,\n",
       "         7.78665158e+01,   8.98665158e+01,   9.58665158e+01,\n",
       "         9.68665158e+01,   3.98665158e+01,  -2.11334842e+01,\n",
       "         8.48665158e+01,  -7.41334842e+01,  -1.71334842e+01,\n",
       "         9.18665158e+01,   4.68665158e+01,   1.17866516e+02,\n",
       "         1.18665158e+01,  -8.01334842e+01,  -5.61334842e+01,\n",
       "         1.53866516e+02,  -6.11334842e+01,   6.18665158e+01,\n",
       "        -5.71334842e+01,   6.38665158e+01,   1.10866516e+02,\n",
       "         2.58665158e+01,  -3.91334842e+01,   4.78665158e+01,\n",
       "        -1.31334842e+01,  -1.31334842e+01,  -6.41334842e+01,\n",
       "        -4.13348416e+00,  -6.41334842e+01,   9.08665158e+01,\n",
       "        -8.11334842e+01,  -7.51334842e+01,  -4.31334842e+01,\n",
       "         1.19866516e+02,  -9.21334842e+01,  -9.81334842e+01,\n",
       "         6.88665158e+01,  -6.21334842e+01,   1.58866516e+02,\n",
       "         1.28866516e+02,   2.98665158e+01,   1.68866516e+02,\n",
       "        -9.41334842e+01,   1.09866516e+02,   5.38665158e+01,\n",
       "         8.08665158e+01,   8.98665158e+01,  -2.91334842e+01,\n",
       "         1.48665158e+01,  -8.91334842e+01,   4.48665158e+01,\n",
       "        -8.11334842e+01,   1.58665158e+01,  -1.21334842e+01,\n",
       "         6.48665158e+01,  -3.11334842e+01,   8.28665158e+01,\n",
       "         9.28665158e+01,  -1.12133484e+02,  -1.00133484e+02,\n",
       "        -4.81334842e+01,  -2.01334842e+01,  -6.41334842e+01,\n",
       "        -8.31334842e+01,   6.68665158e+01,  -8.01334842e+01,\n",
       "         4.88665158e+01,  -4.21334842e+01,  -1.01133484e+02,\n",
       "         1.24866516e+02,  -8.91334842e+01,  -3.41334842e+01,\n",
       "        -8.31334842e+01,   1.20866516e+02,   1.05866516e+02,\n",
       "        -1.09133484e+02,   4.58665158e+01,   8.98665158e+01,\n",
       "         7.98665158e+01,   2.28665158e+01,  -5.91334842e+01,\n",
       "         1.58665158e+01,   1.22866516e+02,   1.40866516e+02,\n",
       "         1.28866516e+02,  -8.01334842e+01,  -1.21334842e+01,\n",
       "         3.68665158e+01,   2.88665158e+01,   5.68665158e+01,\n",
       "        -1.61334842e+01,   1.08866516e+02,  -3.91334842e+01,\n",
       "        -2.11334842e+01,   2.18665158e+01,   1.04866516e+02,\n",
       "        -9.71334842e+01,  -6.81334842e+01,  -1.10133484e+02,\n",
       "        -6.13348416e+00,   5.98665158e+01,   8.08665158e+01,\n",
       "        -6.11334842e+01,  -4.11334842e+01,  -1.33484163e-01,\n",
       "        -3.21334842e+01,  -8.51334842e+01,   1.57866516e+02,\n",
       "        -5.81334842e+01,   3.08665158e+01,  -8.61334842e+01,\n",
       "         2.08665158e+01,  -8.01334842e+01,  -1.03133484e+02,\n",
       "        -8.81334842e+01,  -1.04133484e+02,   2.58665158e+01,\n",
       "        -4.81334842e+01,  -2.01334842e+01,   6.78665158e+01,\n",
       "        -9.51334842e+01])), array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286377, -0.02593034],\n",
       "       ..., \n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04687948,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452837, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00421986,  0.00306441]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X -= X.mean(0)[None,:]; Y -= Y.mean()\n",
    "full_soln = np.linalg.pinv(X).dot(Y)\n",
    "min_norm = np.linalg.norm(Y - X.dot(full_soln))\n",
    "l2_part = rr.l2norm.affine(X, -Y, bound=1.1*min_norm) # we can't take a bound any smaller than sqrt(RSS)\n",
    "l2_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1236.6970603462826, 1618.9530951928127)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_norm*1.1, np.linalg.norm(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem can be turned into a problem solvable by `regreg` if we smooth out `l2_part`. This is \n",
    "related to the approaches taken by `NESTA` and `TFOCS`.\n",
    "\n",
    "There are quite a few variations, but one approach is to smooth the `l2_part` and solve a problem with a smoothed conjugate and an $\\ell_1$ penalty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing out atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\begin{aligned}\n",
       "\\text{minimize}_{\\beta} & f(\\beta) + g(\\beta) \\\\\n",
       "f(\\beta) &=  \\sup_{u \\in \\mathbb{R}^{p} } \\left[ \\langle X_{1}\\beta, u \\rangle - \\left(\\lambda_{1} \\|u\\|_2 + \\frac{L_{1}}{2}\\|u\\|^2_2 + \\left \\langle \\eta_{1}, u \\right \\rangle \\right) \\right] \\\\\n",
       "g(\\beta) &= \\lambda_{2} \\|\\beta\\|_1 \\\\\n",
       "\\end{aligned}\n",
       "$$"
      ],
      "text/plain": [
       "<regreg.problems.simple.simple_problem at 0x10a4eacd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_q1 = rr.identity_quadratic(1.e-4, 0, 0, 0)\n",
    "l2_part_smoothed = l2_part.smoothed(small_q1)\n",
    "smoothed_problem = rr.simple_problem(l2_part_smoothed, l1_part)\n",
    "smoothed_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,   -0.        ,  433.87926454,   78.4822789 ,\n",
       "          0.        ,    0.        ,   -0.        ,    0.        ,\n",
       "        372.99296915,    0.        ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_soln = smoothed_problem.solve(min_its=10000)\n",
    "smoothed_soln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFOCS\n",
    "\n",
    "The TFOCS approach similarly smooths atoms, but solves this by adding a small quadratic \n",
    "to the objective before solving a dual problem. Formally, `TFOCS` solves a sequence of such\n",
    "smoothed problems where the quadratic term is updated along the sequence. The center of the quadratic is also updated\n",
    "along the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\sup_{u \\in \\mathbb{R}^{p} } \\left[ \\langle \\beta, u \\rangle - \\left(\\lambda_{} \\|u\\|_1 + \\frac{L_{}}{2}\\|u\\|^2_2 \\right) \\right]$$"
      ],
      "text/plain": [
       "smooth_conjugate(l1norm((10,), lagrange=1.000000, offset=None, quadratic=identity_quadratic(0.000001, 0.0, 0.0, 0.000000)),identity_quadratic(0.000001, 0.0, 0.0, 0.000000))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_q2 = rr.identity_quadratic(1.e-6, 0, 0, 0)\n",
    "l1_part2 = rr.l1norm(X.shape[1], lagrange=1., quadratic=small_q2)\n",
    "linf_smoothed = l1_part2.conjugate\n",
    "linf_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\begin{aligned}\n",
       "\\text{minimize}_{\\beta} & f(\\beta) + g(\\beta) \\\\\n",
       "f(\\beta) &=  \\sup_{u \\in \\mathbb{R}^{p} } \\left[ \\langle X_{1}\\beta, u \\rangle - \\left(\\lambda_{1} \\|u\\|_1 + \\frac{L_{1}}{2}\\|u\\|^2_2 \\right) \\right] \\\\\n",
       "g(\\beta) &= \\lambda_{2} \\|\\beta\\|_2 + \\left \\langle \\eta_{2}, \\beta \\right \\rangle \\\\\n",
       "\\end{aligned}\n",
       "$$"
      ],
      "text/plain": [
       "<regreg.problems.simple.simple_problem at 0x10a4f30d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from regreg.affine import scalar_multiply, adjoint\n",
    "transform, dual_atom = l2_part.dual\n",
    "full_transform = adjoint(scalar_multiply(transform, -1))\n",
    "tfocs_problem = rr.simple_problem(rr.affine_smooth(linf_smoothed, full_transform), dual_atom)\n",
    "tfocs_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfocs_soln = tfocs_problem.solve(tol=1.e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primal solution is stored in the object `linf_smoothed` as `grad` which was the minimizer\n",
    "for the conjugate function before applying `full_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,   -0.        ,  433.52869006,   78.08975989,\n",
       "          0.        ,    0.        ,   -0.        ,    0.        ,\n",
       "        373.71691565,    0.        ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primal_soln = linf_smoothed.grad\n",
    "primal_soln"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
