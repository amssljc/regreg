{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO\n",
    "\n",
    "This notebook covers various optimization problems related to the LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.loadtxt(\"X.csv\", delimiter=',')\n",
    "Y = np.loadtxt(\"Y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given $X, Y$, here is the squared error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import regreg.api as rr\n",
    "loss = rr.squared_error(X, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `loss` is an instance of `regreg.smooth.affine_smooth` the representation of a smooth function in `regreg` composed with a linear transformation. Its \n",
    "most important API piece is `smooth_objective` which evaluates the function, its gradient or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value, score_at_zero = loss.smooth_objective(np.zeros(loss.shape), 'both')\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_at_zero, X.T.dot(X.dot(np.zeros(loss.shape)) - Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LASSO uses an $\\ell_1$ penalty in \"Lagrange\" form:\n",
    "$$\n",
    "\\text{minimize}_{\\beta} \\frac{1}{2} \\|Y-X\\beta\\|^2_2 + \\lambda \\|\\beta\\|_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "penalty = rr.l1norm(10, lagrange=200.)\n",
    "print ('penalty:', str(penalty))\n",
    "penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object penalty is an instance of `regreg.atoms.seminorm`. The main API used in `regreg`\n",
    "is the method `proximal` which computes the proximal mapping of the object. In `regreg`, an `atom` generally means it has a simple proximal map.\n",
    "\n",
    "The proximal mapping of the function \n",
    "$$\n",
    "f(\\beta) = \\lambda \\|\\beta\\|_1\n",
    "$$\n",
    "is\n",
    "$$\n",
    "\\text{prox}_{f, \\epsilon}(z) = \\text{argmin}_{\\beta} \\left[\\frac{\\epsilon}{2}\\|\\beta-z\\|^2_2 + f(\\beta)\\right].\n",
    "$$\n",
    "\n",
    "See [this document](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf) for a brief review of proximal maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $f$ is as above, this is the soft-thresholding map\n",
    "$$\n",
    "\\text{prox}_{f,\\epsilon}(z)_i = \n",
    "\\begin{cases}\n",
    "\\text{sign}(z_i)(|z_i| - \\lambda / \\epsilon) & |z_i| > \\lambda  / \\epsilon \\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "More generally, we might want to solve\n",
    "$$\n",
    "\\text{minimize}_{\\beta} \\left[\\frac{C}{2} \\|\\beta-\\mu\\|^2_2 + \\eta^T\\beta + \\gamma + f(\\beta)\\right]\n",
    "$$\n",
    "which can easily done if we know the proximal mapping.\n",
    "\n",
    "In `regreg`, objects $Q$ of the form\n",
    "$$\n",
    "Q(\\beta) =  \\frac{C}{2} \\|\\beta-\\mu\\|^2_2 + \\eta^T\\beta + \\gamma\n",
    "$$\n",
    "are represented instances of `rr.identity_quadratic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = np.random.standard_normal(penalty.shape)\n",
    "penalty.lagrange = 0.1\n",
    "epsilon = 0.4\n",
    "quadratic_term = rr.identity_quadratic(epsilon, Z, 0, 0)\n",
    "penalty.proximal(quadratic_term) - penalty.solve(quadratic_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = penalty.lagrange / epsilon\n",
    "soft_thresh_Z = np.sign(Z) * (np.fabs(Z) - threshold) * (np.fabs(Z) > threshold)\n",
    "soft_thresh_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects `loss` and `penalty` are combined to form the LASSO objective above. \n",
    "This is the canonical problem that we want to solve:\n",
    "$$\n",
    "\\text{minimize}_{\\beta} f(\\beta) + g(\\beta)\n",
    "$$\n",
    "where $f$ is a smooth convex function (i.e. we can compute its value and its gradient)\n",
    "and $g$ is a function whose proximal map is easy to compute.\n",
    "\n",
    "The object `rr.simple_problem` requires its first argument to have a `smooth_objective`\n",
    "method and its second argument to have a `solve` method that solves\n",
    "$$\n",
    "\\text{minimize}_{\\beta} g(\\beta) + Q(\\beta)\n",
    "$$\n",
    "where $Q$ is a quadratic of the above form. If $g$ has a `proximal` method, this step\n",
    "just calls the proximal mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "penalty.lagrange = 200.\n",
    "problem_lagrange = rr.simple_problem(loss, penalty)\n",
    "problem_lagrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_lagrange = problem_lagrange.solve(tol=1.e-12)\n",
    "print(coef_lagrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "implied_bound = np.fabs(coef_lagrange).sum()\n",
    "print(implied_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bound_constraint = rr.l1norm(10, bound=implied_bound)\n",
    "bound_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "problem_bound = rr.simple_problem(loss, bound_constraint)\n",
    "problem_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_bound = problem_bound.solve(tol=1.e-12)\n",
    "print(coef_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(coef_bound - coef_lagrange) / np.linalg.norm(coef_lagrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to `sklearn`\n",
    "\n",
    "The objective function is differs from `sklearn.linear_model.Lasso` by a factor of $1/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "clf = Lasso(alpha=penalty.lagrange / X.shape[0])\n",
    "sklearn_soln = clf.fit(X, Y).coef_\n",
    "sklearn_soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtiming = np.random.standard_normal((2000, 4000))\n",
    "Ytiming = np.random.standard_normal(2000)\n",
    "lagrange = np.fabs(Xtiming.T.dot(Ytiming)).max() * 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "clf = Lasso(alpha=lagrange / Xtiming.shape[0])\n",
    "sklearn_soln = clf.fit(Xtiming, Ytiming).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "loss = rr.squared_error(Xtiming, Ytiming)\n",
    "penalty = rr.l1norm(Xtiming.shape[1], lagrange=lagrange)\n",
    "rr.simple_problem(loss,penalty).solve(tol=1.e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_t = rr.squared_error(Xtiming, Ytiming)\n",
    "penalty_t = rr.l1norm(Xtiming.shape[1], lagrange=lagrange)\n",
    "soln1 = rr.simple_problem(loss_t, penalty_t).solve(tol=1.e-6)\n",
    "clf = Lasso(alpha=lagrange / Xtiming.shape[0])\n",
    "soln2 = clf.fit(Xtiming, Ytiming).coef_\n",
    "print (soln1 != 0).sum(), (soln2 != 0).sum()\n",
    "np.linalg.norm(soln1 - soln2) / np.linalg.norm(soln1)\n",
    "(loss_t.smooth_objective(soln1, 'func') + np.fabs(soln1).sum() * lagrange, loss_t.smooth_objective(soln2, 'func') + np.fabs(soln2).sum() * lagrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn_soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(sklearn_soln - coef_lagrange) / np.linalg.norm(coef_lagrange)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net\n",
    "\n",
    "The elastic net differs from the LASSO only by addition of a quadratic term.\n",
    "In `regreg`, both smooth functions and atoms have their own quadratic term that\n",
    "is added to the objective before solving the problem. \n",
    "\n",
    "The `identity_quadratic` is specified as $Q$ above:\n",
    "$$\n",
    "Q(\\beta) = \\frac{C}{2} \\|\\beta-\\mu\\|^2_2 + \\eta^T\\beta + \\gamma\n",
    "$$\n",
    "with $C$ the first argument, $\\mu$ the second, $\\eta$ the third and $\\gamma$ the fourth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enet_term = rr.identity_quadratic(0.5,0,0,0)\n",
    "enet_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "penalty_enet = rr.l1norm(10, lagrange=200., quadratic=enet_term)\n",
    "penalty_enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "problem_enet = rr.simple_problem(loss, penalty_enet)\n",
    "enet_lagrange = problem_enet.solve(min_its=200, tol=1.e-12)\n",
    "enet_lagrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic terms can also be added to problems as the first argument to `solve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "problem_lagrange.solve(enet_term, min_its=200, tol=1.e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Objects like `enet_term` are ubiquitous in `regreg` because it is a package\n",
    "that uses proximal gradient methods to solve problems. Hence, it is repeatedly solving problems like\n",
    "$$\n",
    "\\text{minimize}_{\\beta} \\frac{C}{2} \\|z-\\beta\\|^2_2 + {\\cal P}(\\beta).\n",
    "$$\n",
    "\n",
    "It therefore manipulates these objects in the course of solving the problem.\n",
    "The arguments to `rr.identity_quadratic` determine functions like\n",
    "$$\n",
    "\\beta \\mapsto \\frac{C}{2} \\|\\beta - \\mu\\|^2_2 + \\beta^T\\eta + \\gamma.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = 0.5 \n",
    "mu = np.arange(4)\n",
    "eta = np.ones(4)\n",
    "gamma = 2.3\n",
    "\n",
    "iq = rr.identity_quadratic(C, mu, eta, gamma)\n",
    "str(iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = -np.ones(4)\n",
    "iq.objective(beta, 'func'), 0.5*C*((beta-mu)**2).sum() + (beta*eta).sum() + gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments $\\mu$ is the `center` and $\\eta$ is the `linear_term`, the argument $\\gamma$ is `constant` which seems somewhat unnecessary but is sometimes useful to track through computations.\n",
    "such that `center` is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(iq.collapsed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As atoms and smooth functions have their own such quadratic terms, one sometimes collects\n",
    "them to form an overall quadratic term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iq2 = rr.identity_quadratic(0.3, eta, mu, -2.1)\n",
    "iq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(iq+iq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iq.collapsed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual problems\n",
    "\n",
    "The LASSO or Elastic Net can often be solved by solving an associated dual problem.\n",
    "There are various ways to construct such problems. \n",
    "\n",
    "One such way is to write our elastic net problem as\n",
    "$$\n",
    "\\text{minimize}_{\\beta} f(\\beta) + g(\\beta)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\beta) &= \\frac{1}{2} \\|Y-X\\beta\\|^2_2 + \\frac{C}{2} \\|\\beta\\|^2_2 \\\\\n",
    "g(\\beta) &= \\lambda \\|\\beta\\|_1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then, we duplicate the variable $\\beta$ yielding\n",
    "$$\n",
    "\\text{minimize}_{\\beta_1,\\beta_2:\\beta_1=\\beta_2} f(\\beta_1) + g(\\beta_2)\n",
    "$$\n",
    "and introduce the Lagrangian\n",
    "$$\n",
    "L(\\beta_1,\\beta_2,u) = f(\\beta_1) + g(\\beta_2) + u^T(\\beta_1-\\beta_2).\n",
    "$$\n",
    "\n",
    "The dual problem is constructed by minimizing over $(\\beta_1,\\beta_2)$ which yields a function of\n",
    "$u$:\n",
    "$$\n",
    "\\inf_{\\beta_1,\\beta_2}L(\\beta_1,\\beta_2,u) = -f^*(-u) - g^*(u)\n",
    "$$\n",
    "where \n",
    "$$\n",
    "f^*(u) = \\sup_{\\beta} \\beta^Tu - f(\\beta)\n",
    "$$\n",
    "is the convex conjugate of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual problem, written as a minimization problem is\n",
    "$$\n",
    "\\text{minimize}_{u} f^*(-u) + g^*(u).\n",
    "$$\n",
    "\n",
    "In the elastic net case, \n",
    "$$\n",
    "g^*(u) = I^{\\infty}(\\|u\\|_{\\infty} \\leq \\lambda)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^*(-u) &= -\\inf_{\\beta}\\left[ \\frac{1}{2} \\|Y-X\\beta\\|^2_2 + \\frac{C}{2}\\|\\beta\\|^2_2 + u^T\\beta\\right] \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the optimal $\\beta$ in computing the infimum aboves satisfies the normal equations\n",
    "$$\n",
    "(X^TX + C \\cdot I)\\beta^*(u,Y) = X^TY - u\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\beta^*(u,Y) = (X^TX+C \\cdot I)^{-1}(X^TY-u).\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "f^*(-u) = \\frac{1}{2} (X^TY-u)^T(X^TX+C \\cdot I)^{-1}(X^TY-u) - \\frac{1}{2}\\|Y\\|^2_2.\n",
    "$$\n",
    "\n",
    "The function $f^*$ can be evaluated exactly as it is quadratic, though it can also be solved numerically if \n",
    "our loss was not squared-error. This is what the class `regreg.api.conjugate` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dual_loss = rr.conjugate(loss, negate=True, quadratic=enet_term, tol=1.e-12)\n",
    "Q = np.linalg.inv(X.T.dot(X) + enet_term.coef * np.identity(10))\n",
    "\n",
    "def dual_loss_explicit(u):\n",
    "    z = X.T.dot(Y) - u\n",
    "    return 0.5 * (z * Q.dot(z)).sum() - 0.5 * (Y**2).sum()\n",
    "\n",
    "U = np.random.standard_normal(10) * 1\n",
    "print np.linalg.norm((dual_loss.smooth_objective(U, 'grad') + Q.dot(X.T.dot(Y) - U)))  / np.linalg.norm(dual_loss.smooth_objective(U, 'grad'))\n",
    "print dual_loss.smooth_objective(U, 'func'), dual_loss_explicit(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `negate` option tells `regreg` that the function we want is the conjugate of `loss` composed with\n",
    "a sign change, i.e. a linear transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dual_atom = penalty.conjugate\n",
    "print str(dual_atom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dual_problem = rr.simple_problem(dual_loss, dual_atom)\n",
    "dual_soln = dual_problem.solve(min_its=50)\n",
    "dual_soln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this dual problem is equal to the negative of the gradient of the objective of our \n",
    "elastic net at the solution. This is sometimes referred to as a primal-dual relationship, and is\n",
    "in effect a restatement of the KKT conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "- loss.smooth_objective(enet_lagrange, 'grad') - enet_term.objective(enet_lagrange, 'grad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `conjugate` object, `regreg` retains a reference to the minimizer, i.e. the gradient of the\n",
    "conjugate function. In our problem, this is actually the solution to our elastic net problem, though it\n",
    "does not have exact zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primal_soln = dual_loss.argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primal_soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.linalg.norm(primal_soln - enet_lagrange) / np.linalg.norm(enet_lagrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could alternatively have formed the explicit quadratic function for $f^*(-u)$. Having formed the \n",
    "quadratic objective explicitly, we will have to also explicitly solve for the primal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dual_quadratic = rr.quadratic(Q.shape[0], Q=Q, offset=X.T.dot(Y))\n",
    "dual_problem_alt = rr.simple_problem(dual_quadratic, dual_atom)\n",
    "dual_soln_alt = dual_problem_alt.solve(min_its=100)\n",
    "dual_soln_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primal_soln_alt = -dual_quadratic.smooth_objective(dual_soln_alt, 'grad')\n",
    "print np.linalg.norm(primal_soln_alt - enet_lagrange) / np.linalg.norm(enet_lagrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis pursuit\n",
    "\n",
    "Yet another species in the zoology of LASSO problems is the basis pursuit problem\n",
    "$$\n",
    "\\text{minimize}_{\\beta: \\|y-X\\beta\\|_2 \\leq \\delta} \\|\\beta\\|_1.\n",
    "$$\n",
    "This can be written as the sum of two atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_part = rr.l1norm(X.shape[1], lagrange=1.)\n",
    "l1_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X -= X.mean(0)[None,:]; Y -= Y.mean()\n",
    "full_soln = np.linalg.pinv(X).dot(Y)\n",
    "min_norm = np.linalg.norm(Y - X.dot(full_soln))\n",
    "l2_part = rr.l2norm.affine(X, -Y, bound=1.1*min_norm) # we can't take a bound any smaller than sqrt(RSS)\n",
    "l2_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_norm*1.1, np.linalg.norm(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem can be turned into a problem solvable by `regreg` if we smooth out `l2_part`. This is \n",
    "related to the approaches taken by `NESTA` and `TFOCS`.\n",
    "\n",
    "There are quite a few variations, but one approach is to smooth the `l2_part` and solve a problem with a smoothed conjugate and an $\\ell_1$ penalty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing out atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_q1 = rr.identity_quadratic(1.e-4, 0, 0, 0)\n",
    "l2_part_smoothed = l2_part.smoothed(small_q1)\n",
    "smoothed_problem = rr.simple_problem(l2_part_smoothed, l1_part)\n",
    "smoothed_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smoothed_soln = smoothed_problem.solve(min_its=10000)\n",
    "smoothed_soln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFOCS\n",
    "\n",
    "The TFOCS approach similarly smooths atoms, but solves this by adding a small quadratic \n",
    "to the objective before solving a dual problem. Formally, `TFOCS` solves a sequence of such\n",
    "smoothed problems where the quadratic term is updated along the sequence. The center of the quadratic is also updated\n",
    "along the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_q2 = rr.identity_quadratic(1.e-6, 0, 0, 0)\n",
    "l1_part2 = rr.l1norm(X.shape[1], lagrange=1., quadratic=small_q2)\n",
    "linf_smoothed = l1_part2.conjugate\n",
    "linf_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from regreg.affine import scalar_multiply, adjoint\n",
    "transform, dual_atom = l2_part.dual\n",
    "full_transform = adjoint(scalar_multiply(transform, -1))\n",
    "tfocs_problem = rr.simple_problem(rr.affine_smooth(linf_smoothed, full_transform), dual_atom)\n",
    "tfocs_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfocs_soln = tfocs_problem.solve(tol=1.e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primal solution is stored in the object `linf_smoothed` as `grad` which was the minimizer\n",
    "for the conjugate function before applying `full_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primal_soln = linf_smoothed.grad\n",
    "primal_soln"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
