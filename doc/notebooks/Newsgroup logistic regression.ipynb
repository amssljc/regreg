{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with regreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic loss is included in regreg, taking a pair $(X,Y)$ where $X$ is an affine_transform and $Y$ is either a binary vector or, if not, an additional argument of 'trials' is needed specifying how many trials\n",
    "per row of $Y$. This is equivalent to setting the weights option in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, regreg.api as rr\n",
    "\n",
    "n, p = 70, 8\n",
    "Xr = np.random.standard_normal((n, p))\n",
    "Yr = np.random.binomial(1,0.5, (n,))\n",
    "lossr = rr.logistic_loss(Xr,Yr)\n",
    "coefsr = lossr.solve(coef_stop=True, tol=1.e-8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this with R's output. First, we load the R magic into ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rmagic\n",
    "cr = %R -i Xr -i Yr mylm = glm(Yr~Xr-1, family=binomial()); mylm$coef\n",
    "print np.linalg.norm(coefsr-cr) / np.linalg.norm(cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the objective value. By default, regreg divides the deviance by n. This can be changed by adding the argument \"coef\" to logistic loss which multiplies the objective by coef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lossr.smooth_objective(coefsr, 'func') * n\n",
    "fromR = %R summary(mylm)$deviance\n",
    "\n",
    "loss2 = lossr = rr.logistic_loss(Xr,Yr, coef=n)\n",
    "print loss2.smooth_objective(coefsr, 'func'), fromR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check that using trials is equivalent to using weights. _This is obviously a bug_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trials = np.ones(n, np.float)\n",
    "trials[:20] = 2\n",
    "trials[20:30] = 3.\n",
    "\n",
    "lossw = rr.logistic_loss(Xr,Yr,trials=trials, coef=n)\n",
    "coefsw = lossw.solve(tol=1.e-8)\n",
    "%R -i trials\n",
    "coefsRw = %R glm(Yr ~ Xr - 1, weights=trials, family=binomial())$coef\n",
    "print coefsw.shape\n",
    "print np.linalg.norm(coefsw-coefsRw) / np.linalg.norm(coefsw)\n",
    "print lossw.smooth_objective(coefsw, 'func'), lossw.smooth_objective(coefsRw, 'func')\n",
    "%R summary(glm(Yr~Xr-1, weights=trials, family=binomial()))$deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import for newsgroup example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare *regreg*'s solution to the newsgroup data to *glmnet*. The dataset is large with $(n,p)$=(11314, 777811)   though the design matrix is sparse. Unfortunately, we can't load in sparse matrices directly from R. So, we will write out to .csv save as .mat and use scipy.io.loadmat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib, os, scipy.io, scipy.sparse\n",
    "\n",
    "if not os.path.exists('newsgroup.mat'):\n",
    "    print 'Had to form \"newsgroup.mat\"'\n",
    "    if not os.path.exists('NewsGroup.RData'):\n",
    "        print 'Had to download the data....'\n",
    "        with file('NewsGroup.RData', 'w') as f:\n",
    "            f.write(urllib.urlopen('http://www.jstatsoft.org/v33/i01/supp/6').read())\n",
    "  \n",
    "    %R library(Matrix)\n",
    "    %R load('NewsGroup.RData')\n",
    "    %R newsX = NewsGroup$x\n",
    "    %R newsY = NewsGroup$y\n",
    "    %R writeMM(newsX, 'newsX.mtx')\n",
    "        \n",
    "    X = scipy.io.mmread('newsX.mtx')\n",
    "    Y = %R newsY \n",
    "    scipy.io.savemat('newsgroup.mat', {'X':X, 'Y':Y})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and penalty specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function assumes binary successes (or proportions in [0,1]). We will center and scale our design matrix after having added an intercept to it. By default, scale and center are True for normalize while\n",
    "intercept_column defaults to None. Finally, our loss is logistic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = scipy.io.loadmat('newsgroup.mat')\n",
    "X = D['X']; Y = D['Y']\n",
    "\n",
    "# convert to binary\n",
    "Y = (Y + 1) / 2; Y.shape = -1\n",
    "\n",
    "# add intercept and normalize\n",
    "X1 = scipy.sparse.hstack([np.ones((X.shape[0], 1)), X]).tocsc() # we use csc because we slice columns later\n",
    "Xn = rr.normalize(X1, center=True, scale=True, intercept_column=0)\n",
    "n, p = Xn.output_shape[0], Xn.input_shape[0]\n",
    "\n",
    "# form the loss\n",
    "\n",
    "loss = rr.logistic_loss(Xn, Y, coef=0.5)\n",
    "print n, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are ready to define the penalty: the LASSO penalty, what else? We'll also compute $\\lambda_{\\max}$ the smallest value for which all coefficients are 0. This is the $\\ell_{\\infty}$ norm of the \n",
    "gradient at 0, almost. Actually, it should really be the $\\ell_{\\infty}$ norm of the gradient at the null model, i.e. with just an intercept. However, at an optimal solution the coordinate of the gradient corresponding to the\n",
    "intercept must be 0 (in fact for any value of $\\lambda$) so only the penalized columns enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights=np.ones(p); weights[Xn.intercept_column] = 0;\n",
    "coefs = np.zeros(p)\n",
    "\n",
    "lagrange_max = np.fabs(loss.smooth_objective(coefs, 'grad'))[1:].max()\n",
    "penalty = rr.weighted_l1norm(weights, lagrange=0.7*lagrange_max)\n",
    "lipschitz = rr.power_L(Xn) / n\n",
    "\n",
    "print lipschitz, lagrange_max, loss.coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, this null model does not change $\\lambda_{\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "null_design = np.ones((n,1))\n",
    "null_loss = rr.logistic_loss(null_design, Y)\n",
    "null_coef = null_loss.solve()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefs[0] = null_coef\n",
    "lagrange_max_null = np.fabs(loss.smooth_objective(coefs, 'grad'))[1:].max()\n",
    "print lagrange_max_null, lagrange_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is a simple problem, in the sense that its prox is separable so we can instantiate it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "problem = rr.simple_problem(loss, penalty)\n",
    "problem.lipschitz = lipschitz\n",
    "soln = problem.solve(start_step=lipschitz/1000, debug=True, tol=1.e-9)\n",
    "loss.objective(soln)\n",
    "print 'Final objective value: ', problem.objective(soln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do it, which will turn out to be slightly easier for maintaining the so-called \"active\" set is to think of the penalty on the linear coefficients as one penalty and \n",
    "the \"zero\" penalty on the intercept as a separate penalty. The problem is still separable, we will just explicitly state is as separable with *only* a problem on the linear coefficients. It takes roughly the same number of iterations to solve. We won't use %%timeit because we want to store the output for use in demonstrating the strong rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "linear_slice = slice(1, Xn.input_shape[0])\n",
    "linear_penalty = rr.l1norm(p-1, lagrange=0.7*lagrange_max)\n",
    "separable = rr.separable_problem(loss, Xn.input_shape, [linear_penalty], [linear_slice])\n",
    "separable.coefs[0] = null_coef\n",
    "final_inv_step = lipschitz / 1000\n",
    "\n",
    "separable_soln = separable.solve(start_step=final_inv_step, tol=1.e-9,debug=True)\n",
    "print 'Final objective value: ', separable.objective(separable_soln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strong rules take a current solution, with its active variables try to guess which variables will enter the model at a new value of the \n",
    "Lagrange parameter. It does this by guessing a bound on the slope of the dual paths. Typically this value is 1, but it could be other values as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the so-called strong rules (see http://arxiv.org/pdf/1011.2234.pdf) to screen variables at each step.\n",
    "The rule takes the gradient of the smooth part of the problem at $\\lambda_{\\text{cur}}$ and tries to guess which variables will still be excluded at $\\lambda_{\\text{new}} < \\lambda_{\\text{cur}}$.\n",
    "There are also possibly some unpenalized columns, these are ignored as the KKT conditions say that those entries of the gradient must be 0 at a minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strong_set_lasso(grad, penalized, lagrange_cur, lagrange_new, slope_estimate=1):\n",
    "    if not isinstance(penalized, rr.selector):\n",
    "        s = rr.selector(penalized, grad.shape)\n",
    "    else:\n",
    "        s = penalized\n",
    "    value = np.zeros(grad.shape, np.bool)\n",
    "    value += (s.adjoint_map(np.fabs(s.linear_map(grad)) < (slope_estimate+1) \\\n",
    "                                * lagrange_new - slope_estimate*lagrange_cur) >= 0)\n",
    "    return ~value\n",
    "\n",
    "g = loss.smooth_objective(separable_soln, 'grad')\n",
    "linear_selector = separable.selectors[0]\n",
    "strong_set = strong_set_lasso(g, linear_selector, 0.7 * lagrange_max, 0.6 * lagrange_max)\n",
    "print strong_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing a strong set means the problem can be solved much faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def restricted_problem(Xn, Y, candidate_set, lagrange):\n",
    "    '''\n",
    "    Assumes the candidate set includes intercept as first column.\n",
    "    '''\n",
    "    print candidate_set.sum()\n",
    "    Xslice = Xn.slice_columns(candidate_set)\n",
    "    Xslice.intercept_column = 0\n",
    "    loss = rr.logistic_loss(Xslice, Y, coef=0.5)\n",
    "    linear_slice = slice(1, Xslice.input_shape[0])\n",
    "    linear_penalty = rr.l1norm(Xslice.input_shape[0]-1, lagrange=lagrange)\n",
    "    candidate_selector = rr.selector(candidate_set, Xn.input_shape)\n",
    "    penalized_selector = rr.selector(candidate_set[1:], Xn.input_shape)\n",
    "    problem_sliced = rr.separable_problem(loss, Xslice.input_shape, [linear_penalty], [linear_slice])\n",
    "    return problem_sliced, candidate_selector, penalized_selector\n",
    "\n",
    "%timeit sp = restricted_problem(Xn, Y, strong_set, 0.6 * lagrange_max)[0]; sp.solve(start_inv_step=lipschitz / 1000, tol=1.e-9)\n",
    "\n",
    "subproblem, strong_selector, penalized_selector = restricted_problem(Xn, Y, strong_set, 0.6 * lagrange_max)\n",
    "sub_soln = subproblem.solve(start_inv_step=lipschitz / 1000, tol=1.e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to one solution using all coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "linear_penalty.lagrange = 0.6 * lagrange_max \n",
    "separable = rr.separable_problem(loss, Xn.primal_shape, [linear_penalty], [linear_slice])\n",
    "separable.coefs[0] = null_coef\n",
    "final_inv_step = lipschitz / 1000\n",
    "\n",
    "separable_soln[:] = separable.solve(start_inv_step=final_inv_step, tol=1.e-9,debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that solving the problem with all the coefficients whose nonzero coefficients are contained in the strong set. Hence we know that solving the problem with fewer coefficients actually solves the bigger problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(np.nonzero(separable_soln != 0)[0]).issubset(strong_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we have to check whether solving the problem on the candidate strong set and setting the rest to zero\n",
    " solves the entire problem. In other words, we need to check the KKT conditions are satisfied.\n",
    "If we let  be the solution that is 0 everywhere outside the strong set and\n",
    " agrees with the subproblem on the strong set, then we must check that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "|\\nabla L(\\hat{\\beta}_{\\text{sub}})_i| &< \\lambda \\quad & i  \\in \\text{strong}^c \\cap \\text{penalized} \\\\\n",
    "\\nabla L(\\hat{\\beta}_{\\text{sub}})_i &=\\lambda \\sgn(\\nabla L(\\hat{\\beta}_{\\text{sub}})_i)& i  \\in \\text{strong} \\cap \\text{penalized} \\\\\n",
    "\\nabla L(\\hat{\\beta}_{\\text{sub}})_i &= 0 & i  \\in \\text{strong} \\cap \\text{penalized}^c \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "In this function, however, we assume that the unpenalized coefficients have been solved sufficiently and only check the penalized ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_KKT(grad, penalized, solution, lagrange, tol=1.0e-02):\n",
    "    '''\n",
    "    Verify that the KKT conditions for the LASSO possibly with unpenalized coefficients\n",
    "    is satisfied for (grad, solution) where grad is the gradient of the loss evaluated\n",
    "    at solution.\n",
    "    '''\n",
    "    if not isinstance(penalized, rr.selector):\n",
    "        s = rr.selector(penalized, grad.shape)\n",
    "    else:\n",
    "        s = penalized\n",
    "    soln_s = s.linear_map(solution)\n",
    "    g_s = s.linear_map(grad)\n",
    "    failing_s = np.zeros(g_s.shape)\n",
    "    failing = np.zeros(grad.shape, np.bool)\n",
    "\n",
    "    # Check the inactive coefficients\n",
    "    failing += s.adjoint_map(np.fabs(g_s) > lagrange * (1 + tol))\n",
    "\n",
    "    # Check the active coefficients\n",
    "    active = soln_s != 0\n",
    "    failing_s[active] += np.fabs(g_s[active] / lagrange + np.sign(soln_s[active])) >= tol \n",
    "    failing += s.adjoint_map(failing_s)\n",
    "    return failing\n",
    "\n",
    "expanded_soln = strong_selector.adjoint_map(sub_soln)\n",
    "full_grad = loss.smooth_objective(expanded_soln, 'grad')\n",
    "failing = check_KKT(full_grad, penalized_selector, expanded_soln, linear_penalty.lagrange)\n",
    "print failing.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have the makings of a full algorithm using the strong rules. Given a candidate\n",
    " active set, we solve the restricted problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(grad_cur, penalized, Xn, Y, soln_cur, lagrange_cur, lagrange_new, active, start_inv_step=1, tol=1.e-6, slope_estimate=2, coef_stop=True, debug=False):\n",
    "    \n",
    "    # try to solve the problem with the active set\n",
    "    active_subproblem, active_selector, pen_active_selector = restricted_problem(Xn, Y, active, lagrange_new)\n",
    "    active_subproblem.coefs[:] = active_selector.linear_map(soln_cur)\n",
    "    active_soln = active_subproblem.solve(start_inv_step=start_inv_step, coef_stop=coef_stop, tol=tol, debug=debug)\n",
    "    soln_cur[:] = active_selector.adjoint_map(active_soln)\n",
    "        \n",
    "    final_inv_step = active_subproblem.final_inv_step\n",
    "    strong = strong_set_lasso(grad_cur, penalized, lagrange_cur, lagrange_new, slope_estimate=slope_estimate)\n",
    "    #strong_subproblem, strong_selector, pen_strong_selector = restricted_problem(Xn, Y, strong, lagrange_new)\n",
    "    #strong_subproblem.coefs[:] = strong_selector.linear_map(soln_cur)\n",
    "    \n",
    "    #grad_cur = strong_selector.adjoint_map(strong_subproblem.smooth_objective(strong_subproblem.coefs, 'grad'))\n",
    "    #soln_cur[:] = strong_selector.adjoint_map(strong_subproblem.coefs)\n",
    "    # are any strong coefficients failing?\n",
    "    #strong_failing = check_KKT(grad_cur,\n",
    "    #                           pen_strong_selector,\n",
    "    #                           soln_cur,\n",
    "    #                           lagrange_new)\n",
    "    #if strong_failing.sum():\n",
    "    #    failing = strong_selector.adjoint_map(strong_failing)\n",
    "    #    return failing + active, final_inv_step, strong\n",
    "    return soln_cur != 0, final_inv_step, strong\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "def main():\n",
    "\n",
    "    lagrange_sequence = lagrange_max * np.exp(np.linspace(np.log(0.05), 0, 100))[::-1]\n",
    "\n",
    "    # scaling will be needed to get coefficients on original scale                                                                                   \\\n",
    "                                                                                                                                                      \n",
    "\n",
    "    scalings = np.asarray(Xn.col_stds).reshape(-1)\n",
    "    final_inv_step = lipschitz / 1000\n",
    "    # first solution corresponding to all zeros except intercept                                                                                     \\\n",
    "                                                                                                                                                      \n",
    "\n",
    "    solution = np.zeros(p)\n",
    "    solution[0] = null_coef\n",
    "\n",
    "    penalized = np.ones(solution.shape, np.bool)\n",
    "    penalized[0] = False\n",
    "    grad = loss.smooth_objective(solution, 'grad')\n",
    "    strong = strong_set_lasso(grad, penalized, lagrange_sequence[0], lagrange_sequence[1])\n",
    "    active = strong.copy()\n",
    "\n",
    "    solutions = [solution.copy()]\n",
    "    rescaled_solutions = [solution.copy()[1:]]\n",
    "    objective = [loss.smooth_objective(solution, 'func')]\n",
    "    dfs = [1]\n",
    "    retry_counter = 0\n",
    "    import time\n",
    "    toc = time.time()\n",
    "    for lagrange_new, lagrange_cur in zip(lagrange_sequence[1:], lagrange_sequence[:-1]):\n",
    "        num_tries = 0\n",
    "        debug = False\n",
    "        tol = 1.0e-5\n",
    "        while True:\n",
    "            active_new, final_inv_step, strong = fit(grad, penalized, Xn,\n",
    "                                             Y, solution, lagrange_cur,\n",
    "                                             lagrange_new, active,\n",
    "                                             tol=tol,\n",
    "                                             start_inv_step=final_inv_step,\n",
    "                                             debug=debug)\n",
    "            grad = loss.smooth_objective(solution, 'grad')\n",
    "            if active_new.sum() <= active.sum() and (~active * active_new).sum() == 0:\n",
    "                failing = check_KKT(grad, penalized, solution, lagrange_new)\n",
    "                if not failing.sum():\n",
    "                    active = (solution != 0) + active\n",
    "                    break\n",
    "                else:\n",
    "                    retry_counter += 1\n",
    "                    print 'trying again:', retry_counter, 'failing:', np.nonzero(failing)[0], active.sum()\n",
    "                    active += strong\n",
    "            else:\n",
    "                print 'active set different', np.nonzero(active), np.nonzero(active_new)\n",
    "                active = active_new + strong \n",
    "                \n",
    "                \n",
    "            tol /= 2.\n",
    "            num_tries += 1\n",
    "            if num_tries % 5 == 0:\n",
    "                debug=True\n",
    "                tol = 1.0e-5\n",
    "                active += strong_set_lasso(grad, penalized, lagrange_cur, lagrange_new)\n",
    "\n",
    "        solutions.append(solution.copy())\n",
    "        rescaled_solutions.append(solution[1:] / scalings[1:])\n",
    "        objective.append(loss.smooth_objective(solution, mode='func'))\n",
    "        dfs.append(active.shape[0])\n",
    "        print lagrange_cur / lagrange_max, lagrange_new, (solution != 0).sum(), 1. - objective[-1] / objective[0], list(lagrange_sequence).index(lagrange_new), np.fabs(rescaled_solutions[-1]).sum()\n",
    "        gc.collect()\n",
    "        tic = time.time()\n",
    "    \n",
    "        print 'time: %0.1f' % (tic-toc)\n",
    "    solutions = scipy.sparse.lil_matrix(solutions)\n",
    "    rescaled_solutions = scipy.sparse.lil_matrix(rescaled_solutions)\n",
    "    objective = np.array(objective)\n",
    "    output = {'devratio': 1 - objective / objective.max(),\n",
    "              'df': dfs,\n",
    "              'beta': solutions,\n",
    "              'lagrange': lagrange_sequence,\n",
    "              'scalings': scalings,\n",
    "              'rescaled_beta': rescaled_solutions}\n",
    "   \n",
    "    scipy.io.savemat('newsgroup_results.mat', output)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally define our problem, by restricting interest to only some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(glmnet)\n",
    "library(Matrix)\n",
    "load(\"NewsGroup.RData\")\n",
    "\n",
    "newsX=NewsGroup$x\n",
    "newsy=NewsGroup$y\n",
    "\n",
    "\n",
    "x=newsX\n",
    "y=(newsy+1)/2\n",
    "n=nrow(newsX)\n",
    "p=ncol(newsX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "%R a=glmnet(x,as.factor(y),stand=TRUE,family=\"binomial\",lambda.min=0.05); print(a$names)\n",
    "\n",
    "tic = time.time()\n",
    "print 'R time: %0.1f ' % (tic-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "newsgroupFit=list(a=a)\n",
    "save(newsgroupFit,file=\"newsgroupFit.RData\")\n",
    "l1norm = apply(abs(a$beta), 2, sum)\n",
    "write.table(data.frame(lagrange=a$lambda, devratio=a$dev.ratio, df=a$df, l1=l1norm), 'newsgroup_output.csv',row.names=FALSE, sep=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
