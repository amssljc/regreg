{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with regreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic loss is included in regreg, taking a pair $(X,Y)$ where $X$ is an affine_transform and $Y$ is either a binary vector or, if not, an additional argument of 'trials' is needed specifying how many trials\n",
    "per row of $Y$. This is equivalent to setting the weights option in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jb/code/regreg/regreg/smooth/__init__.py:235: RuntimeWarning: divide by zero encountered in log\n",
      "  deviance_terms = np.log(saturated) * self.successes + np.log(1-saturated) * (self.trials - self.successes)\n",
      "/home/jb/code/regreg/regreg/smooth/__init__.py:235: RuntimeWarning: invalid value encountered in multiply\n",
      "  deviance_terms = np.log(saturated) * self.successes + np.log(1-saturated) * (self.trials - self.successes)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, regreg.api as rr\n",
    "\n",
    "n, p = 70, 8\n",
    "Xr = np.random.standard_normal((n, p))\n",
    "Yr = np.random.binomial(1,0.5, (n,))\n",
    "lossr = rr.logistic_loss(Xr,Yr)\n",
    "coefsr = lossr.solve(coef_stop=True, tol=1.e-8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this with R's output. First, we load the R magic into ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27750430224e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jb/.virtualenvs/py2/local/lib/python2.7/site-packages/IPython/extensions/rmagic.py:693: UserWarning: The rmagic extension in IPython is deprecated in favour of rpy2.ipython. If available, that will be loaded instead.\n",
      "http://rpy.sourceforge.net/\n",
      "  warnings.warn(\"The rmagic extension in IPython is deprecated in favour of \"\n"
     ]
    }
   ],
   "source": [
    "%load_ext rmagic\n",
    "cr = %R -i Xr -i Yr mylm = glm(Yr~Xr-1, family=binomial()); mylm$coef\n",
    "print np.linalg.norm(coefsr-cr) / np.linalg.norm(cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the objective value. By default, regreg divides the deviance by n. This can be changed by adding the argument \"coef\" to logistic loss which multiplies the objective by coef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.833182282\n",
      "85.833182282 [ 85.83318228]\n"
     ]
    }
   ],
   "source": [
    "print lossr.smooth_objective(coefsr, 'func') * n\n",
    "fromR = %R summary(mylm)$deviance\n",
    "\n",
    "loss2 = lossr = rr.logistic_loss(Xr,Yr, coef=n)\n",
    "print loss2.smooth_objective(coefsr, 'func'), fromR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check that using trials is equivalent to using weights. _This is obviously a bug_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "1.44337163925\n",
      "146.665234171 160.985281082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 133.12505288])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = np.ones(n, np.float)\n",
    "trials[:20] = 2\n",
    "trials[20:30] = 3.\n",
    "\n",
    "lossw = rr.logistic_loss(Xr,Yr,trials=trials, coef=n)\n",
    "coefsw = lossw.solve(tol=1.e-8)\n",
    "%R -i trials\n",
    "coefsRw = %R glm(Yr ~ Xr - 1, weights=trials, family=binomial())$coef\n",
    "print coefsw.shape\n",
    "print np.linalg.norm(coefsw-coefsRw) / np.linalg.norm(coefsw)\n",
    "print lossw.smooth_objective(coefsw, 'func'), lossw.smooth_objective(coefsRw, 'func')\n",
    "%R summary(glm(Yr~Xr-1, weights=trials, family=binomial()))$deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import for newsgroup example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare *regreg*'s solution to the newsgroup data to *glmnet*. The dataset is large with $(n,p)$=(11314, 777811)   though the design matrix is sparse. Unfortunately, we can't load in sparse matrices directly from R. So, we will write out to .csv save as .mat and use scipy.io.loadmat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib, os, scipy.io, scipy.sparse\n",
    "\n",
    "if not os.path.exists('newsgroup.mat'):\n",
    "    print 'Had to form \"newsgroup.mat\"'\n",
    "    if not os.path.exists('NewsGroup.RData'):\n",
    "        print 'Had to download the data....'\n",
    "        with file('NewsGroup.RData', 'w') as f:\n",
    "            f.write(urllib.urlopen('http://www.jstatsoft.org/v33/i01/supp/6').read())\n",
    "  \n",
    "    %R library(Matrix)\n",
    "    %R load('NewsGroup.RData')\n",
    "    %R newsX = NewsGroup$x\n",
    "    %R newsY = NewsGroup$y\n",
    "    %R writeMM(newsX, 'newsX.mtx')\n",
    "        \n",
    "    X = scipy.io.mmread('newsX.mtx')\n",
    "    Y = %R newsY \n",
    "    scipy.io.savemat('newsgroup.mat', {'X':X, 'Y':Y})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and penalty specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function assumes binary successes (or proportions in [0,1]). We will center and scale our design matrix after having added an intercept to it. By default, scale and center are True for normalize while\n",
    "intercept_column defaults to None. Finally, our loss is logistic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 777812\n"
     ]
    }
   ],
   "source": [
    "D = scipy.io.loadmat('newsgroup.mat')\n",
    "X = D['X']; Y = D['Y']\n",
    "\n",
    "# convert to binary\n",
    "Y = (Y + 1) / 2; Y.shape = -1\n",
    "\n",
    "# add intercept and normalize\n",
    "X1 = scipy.sparse.hstack([np.ones((X.shape[0], 1)), X]).tocsc() # we use csc because we slice columns later\n",
    "Xn = rr.normalize(X1, center=True, scale=True, intercept_column=0)\n",
    "n, p = Xn.output_shape[0], Xn.input_shape[0]\n",
    "\n",
    "# form the loss\n",
    "\n",
    "loss = rr.logistic_loss(Xn, Y, coef=0.5)\n",
    "print n, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are ready to define the penalty: the LASSO penalty, what else? We'll also compute $\\lambda_{\\max}$ the smallest value for which all coefficients are 0. This is the $\\ell_{\\infty}$ norm of the \n",
    "gradient at 0, almost. Actually, it should really be the $\\ell_{\\infty}$ norm of the gradient at the null model, i.e. with just an intercept. However, at an optimal solution the coordinate of the gradient corresponding to the\n",
    "intercept must be 0 (in fact for any value of $\\lambda$) so only the penalized columns enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6306.51725468 0.119658377631 4.41930351777e-05\n"
     ]
    }
   ],
   "source": [
    "weights=np.ones(p); weights[Xn.intercept_column] = 0;\n",
    "coefs = np.zeros(p)\n",
    "\n",
    "lagrange_max = np.fabs(loss.smooth_objective(coefs, 'grad'))[1:].max()\n",
    "penalty = rr.weighted_l1norm(weights, lagrange=0.7*lagrange_max)\n",
    "lipschitz = rr.power_L(Xn) / n\n",
    "\n",
    "print lipschitz, lagrange_max, loss.coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, this null model does not change $\\lambda_{\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "null_design = np.ones((n,1))\n",
    "null_loss = rr.logistic_loss(null_design, Y)\n",
    "null_coef = null_loss.solve()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.119658377631 0.119658377631\n"
     ]
    }
   ],
   "source": [
    "coefs[0] = null_coef\n",
    "lagrange_max_null = np.fabs(loss.smooth_objective(coefs, 'grad'))[1:].max()\n",
    "print lagrange_max_null, lagrange_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is a simple problem, in the sense that its prox is separable so we can instantiate it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Decreasing step to 5.73319750425\n",
      "0    Decreasing step to 5.21199773114\n",
      "0    Decreasing step to 4.73817975558\n",
      "0    Decreasing step to 4.30743614144\n",
      "0    Decreasing step to 3.91585103767\n",
      "0    Decreasing step to 3.5598645797\n",
      "0    Decreasing step to 3.236240527\n",
      "0    Decreasing step to 2.94203684273\n",
      "0    Decreasing step to 2.67457894793\n",
      "0    Decreasing step to 2.43143540721\n",
      "0    Decreasing step to 2.21039582474\n",
      "0    Decreasing step to 2.00945074976\n",
      "0    Decreasing step to 1.82677340887\n",
      "0    obj: 6.931472e-01    step: 1.83e+00    rel_obj_change: 2.45e-03    tol: 1.0e-09\n",
      "1    obj: 6.906982e-01    step: 1.83e+00    rel_obj_change: 4.91e-04    tol: 1.0e-09\n",
      "2    obj: 6.902070e-01    step: 1.83e+00    rel_obj_change: 2.97e-04    tol: 1.0e-09\n",
      "3    obj: 6.899102e-01    step: 1.83e+00    rel_obj_change: 1.64e-04    tol: 1.0e-09\n",
      "4    obj: 6.897464e-01    step: 1.83e+00    rel_obj_change: 9.71e-05    tol: 1.0e-09\n",
      "5    obj: 6.896492e-01    step: 1.83e+00    rel_obj_change: 1.31e-05    tol: 1.0e-09\n",
      "6    obj: 6.896361e-01    step: 1.83e+00    rel_obj_change: 2.62e-06    tol: 1.0e-09\n",
      "6 Restarting weights\n",
      "7    obj: 6.896361e-01    step: 2.01e+00    rel_obj_change: 6.38e-07    tol: 1.0e-09\n",
      "8    obj: 6.896355e-01    step: 2.21e+00    rel_obj_change: 1.62e-07    tol: 1.0e-09\n",
      "9    obj: 6.896353e-01    step: 2.43e+00    rel_obj_change: 5.21e-08    tol: 1.0e-09\n",
      "10    obj: 6.896353e-01    step: 2.67e+00    rel_obj_change: 1.39e-08    tol: 1.0e-09\n",
      "11    obj: 6.896353e-01    step: 2.94e+00    rel_obj_change: 4.30e-09    tol: 1.0e-09\n",
      "12    obj: 6.896353e-01    step: 3.24e+00    rel_obj_change: 8.71e-10    tol: 1.0e-09\n",
      "Success: Optimization stopped because decrease in objective was below tolerance\n",
      "FISTA used 12 of 10000 iterations\n",
      "Final objective value:  0.689635259522\n",
      "1 loops, best of 1: 1.84 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "problem = rr.simple_problem(loss, penalty)\n",
    "problem.lipschitz = lipschitz\n",
    "soln = problem.solve(start_step=lipschitz/1000, debug=True, tol=1.e-9)\n",
    "loss.objective(soln)\n",
    "print 'Final objective value: ', problem.objective(soln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do it, which will turn out to be slightly easier for maintaining the so-called \"active\" set is to think of the penalty on the linear coefficients as one penalty and \n",
    "the \"zero\" penalty on the intercept as a separate penalty. The problem is still separable, we will just explicitly state is as separable with *only* a problem on the linear coefficients. It takes roughly the same number of iterations to solve. We won't use %%timeit because we want to store the output for use in demonstrating the strong rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Decreasing step to 5.73319750425\n",
      "0    Decreasing step to 5.21199773114\n",
      "0    Decreasing step to 4.73817975558\n",
      "0    Decreasing step to 4.30743614144\n",
      "0    Decreasing step to 3.91585103767\n",
      "0    Decreasing step to 3.5598645797\n",
      "0    Decreasing step to 3.236240527\n",
      "0    Decreasing step to 2.94203684273\n",
      "0    Decreasing step to 2.67457894793\n",
      "0    Decreasing step to 2.43143540721\n",
      "0    Decreasing step to 2.21039582474\n",
      "0    Decreasing step to 2.00945074976\n",
      "0    Decreasing step to 1.82677340887\n",
      "0    Decreasing step to 1.66070309898\n",
      "0    obj: 6.922694e-01    step: 1.66e+00    rel_obj_change: 1.85e-03    tol: 1.0e-09\n",
      "1    obj: 6.904218e-01    step: 1.66e+00    rel_obj_change: 2.51e-04    tol: 1.0e-09\n",
      "2    obj: 6.901713e-01    step: 1.66e+00    rel_obj_change: 2.26e-04    tol: 1.0e-09\n",
      "3    obj: 6.899452e-01    step: 1.66e+00    rel_obj_change: 1.66e-04    tol: 1.0e-09\n",
      "4    obj: 6.897793e-01    step: 1.66e+00    rel_obj_change: 1.07e-04    tol: 1.0e-09\n",
      "5    obj: 6.896723e-01    step: 1.66e+00    rel_obj_change: 3.63e-05    tol: 1.0e-09\n",
      "6    obj: 6.896359e-01    step: 1.66e+00    rel_obj_change: 1.78e-06    tol: 1.0e-09\n",
      "6 Restarting weights\n",
      "7    obj: 6.896359e-01    step: 1.83e+00    rel_obj_change: 4.21e-07    tol: 1.0e-09\n",
      "8    obj: 6.896355e-01    step: 2.01e+00    rel_obj_change: 1.55e-07    tol: 1.0e-09\n",
      "9    obj: 6.896354e-01    step: 2.21e+00    rel_obj_change: 6.96e-08    tol: 1.0e-09\n",
      "10    obj: 6.896353e-01    step: 2.43e+00    rel_obj_change: 2.60e-08    tol: 1.0e-09\n",
      "11    obj: 6.896353e-01    step: 2.67e+00    rel_obj_change: 9.34e-09    tol: 1.0e-09\n",
      "12    obj: 6.896353e-01    step: 2.94e+00    rel_obj_change: 2.47e-09    tol: 1.0e-09\n",
      "13    obj: 6.896353e-01    step: 3.24e+00    rel_obj_change: 1.97e-11    tol: 1.0e-09\n",
      "Success: Optimization stopped because decrease in objective was below tolerance\n",
      "FISTA used 13 of 10000 iterations\n",
      "Final objective value:  0.689635258771\n"
     ]
    }
   ],
   "source": [
    "\n",
    "linear_slice = slice(1, Xn.input_shape[0])\n",
    "linear_penalty = rr.l1norm(p-1, lagrange=0.7*lagrange_max)\n",
    "separable = rr.separable_problem(loss, Xn.input_shape, [linear_penalty], [linear_slice])\n",
    "separable.coefs[0] = null_coef\n",
    "final_inv_step = lipschitz / 1000\n",
    "\n",
    "separable_soln = separable.solve(start_step=final_inv_step, tol=1.e-9,debug=True)\n",
    "print 'Final objective value: ', separable.objective(separable_soln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strong rules take a current solution, with its active variables try to guess which variables will enter the model at a new value of the \n",
    "Lagrange parameter. It does this by guessing a bound on the slope of the dual paths. Typically this value is 1, but it could be other values as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the so-called strong rules (see http://arxiv.org/pdf/1011.2234.pdf) to screen variables at each step.\n",
    "The rule takes the gradient of the smooth part of the problem at $\\lambda_{\\text{cur}}$ and tries to guess which variables will still be excluded at $\\lambda_{\\text{new}} < \\lambda_{\\text{cur}}$.\n",
    "There are also possibly some unpenalized columns, these are ignored as the KKT conditions say that those entries of the gradient must be 0 at a minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def strong_set_lasso(grad, penalized, lagrange_cur, lagrange_new, slope_estimate=1):\n",
    "    if not isinstance(penalized, rr.selector):\n",
    "        s = rr.selector(penalized, grad.shape)\n",
    "    else:\n",
    "        s = penalized\n",
    "    value = np.zeros(grad.shape, np.bool)\n",
    "    value += (s.adjoint_map(np.fabs(s.linear_map(grad)) < (slope_estimate+1) \\\n",
    "                                * lagrange_new - slope_estimate*lagrange_cur) >= 0)\n",
    "    return ~value\n",
    "\n",
    "g = loss.smooth_objective(separable_soln, 'grad')\n",
    "linear_selector = separable.selectors[0]\n",
    "strong_set = strong_set_lasso(g, linear_selector, 0.7 * lagrange_max, 0.6 * lagrange_max)\n",
    "print strong_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing a strong set means the problem can be solved much faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f6ee71e45771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mproblem_sliced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_selector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalized_selector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'timeit sp = restricted_problem(Xn, Y, strong_set, 0.6 * lagrange_max)[0]; sp.solve(start_inv_step=lipschitz / 1000, tol=1.e-9)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0msubproblem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrong_selector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalized_selector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestricted_problem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrong_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.6\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlagrange_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jb/.virtualenvs/py2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2305\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2306\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2307\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2309\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jb/.virtualenvs/py2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2226\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2227\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2228\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2229\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32m/home/jb/.virtualenvs/py2/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jb/.virtualenvs/py2/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m   1034\u001b[0m             \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m                 \u001b[0mtime_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m                 \u001b[0mworst_tuning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworst_tuning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jb/.virtualenvs/py2/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-f6ee71e45771>\u001b[0m in \u001b[0;36mrestricted_problem\u001b[1;34m(Xn, Y, candidate_set, lagrange)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogistic_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlinear_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXslice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mlinear_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXslice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlagrange\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlagrange\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mcandidate_selector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mpenalized_selector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jb/code/regreg/regreg/atoms/seminorms.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, shape, lagrange, bound, offset, quadratic, initial)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         atom.__init__(self, shape, offset,\n\u001b[1;32m---> 36\u001b[1;33m                       quadratic, initial)\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbound\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlagrange\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jb/code/regreg/regreg/problems/composite.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, shape, offset, quadratic, initial)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "def restricted_problem(Xn, Y, candidate_set, lagrange):\n",
    "    '''\n",
    "    Assumes the candidate set includes intercept as first column.\n",
    "    '''\n",
    "    print candidate_set.sum()\n",
    "    Xslice = Xn.slice_columns(candidate_set)\n",
    "    Xslice.intercept_column = 0\n",
    "    loss = rr.logistic_loss(Xslice, Y, coef=0.5)\n",
    "    linear_slice = slice(1, Xslice.input_shape[0])\n",
    "    linear_penalty = rr.l1norm(Xslice.input_shape[0]-1, lagrange=lagrange)\n",
    "    candidate_selector = rr.selector(candidate_set, Xn.input_shape)\n",
    "    penalized_selector = rr.selector(candidate_set[1:], Xn.input_shape)\n",
    "    problem_sliced = rr.separable_problem(loss, Xslice.input_shape, [linear_penalty], [linear_slice])\n",
    "    return problem_sliced, candidate_selector, penalized_selector\n",
    "\n",
    "%timeit sp = restricted_problem(Xn, Y, strong_set, 0.6 * lagrange_max)[0]; sp.solve(start_inv_step=lipschitz / 1000, tol=1.e-9)\n",
    "\n",
    "subproblem, strong_selector, penalized_selector = restricted_problem(Xn, Y, strong_set, 0.6 * lagrange_max)\n",
    "sub_soln = subproblem.solve(start_inv_step=lipschitz / 1000, tol=1.e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to one solution using all coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "linear_penalty.lagrange = 0.6 * lagrange_max \n",
    "separable = rr.separable_problem(loss, Xn.primal_shape, [linear_penalty], [linear_slice])\n",
    "separable.coefs[0] = null_coef\n",
    "final_inv_step = lipschitz / 1000\n",
    "\n",
    "separable_soln[:] = separable.solve(start_inv_step=final_inv_step, tol=1.e-9,debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that solving the problem with all the coefficients whose nonzero coefficients are contained in the strong set. Hence we know that solving the problem with fewer coefficients actually solves the bigger problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(np.nonzero(separable_soln != 0)[0]).issubset(strong_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we have to check whether solving the problem on the candidate strong set and setting the rest to zero\n",
    " solves the entire problem. In other words, we need to check the KKT conditions are satisfied.\n",
    "If we let  be the solution that is 0 everywhere outside the strong set and\n",
    " agrees with the subproblem on the strong set, then we must check that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "|\\nabla L(\\hat{\\beta}_{\\text{sub}})_i| &< \\lambda \\quad & i  \\in \\text{strong}^c \\cap \\text{penalized} \\\\\n",
    "\\nabla L(\\hat{\\beta}_{\\text{sub}})_i &=\\lambda \\sgn(\\nabla L(\\hat{\\beta}_{\\text{sub}})_i)& i  \\in \\text{strong} \\cap \\text{penalized} \\\\\n",
    "\\nabla L(\\hat{\\beta}_{\\text{sub}})_i &= 0 & i  \\in \\text{strong} \\cap \\text{penalized}^c \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "In this function, however, we assume that the unpenalized coefficients have been solved sufficiently and only check the penalized ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_KKT(grad, penalized, solution, lagrange, tol=1.0e-02):\n",
    "    '''\n",
    "    Verify that the KKT conditions for the LASSO possibly with unpenalized coefficients\n",
    "    is satisfied for (grad, solution) where grad is the gradient of the loss evaluated\n",
    "    at solution.\n",
    "    '''\n",
    "    if not isinstance(penalized, rr.selector):\n",
    "        s = rr.selector(penalized, grad.shape)\n",
    "    else:\n",
    "        s = penalized\n",
    "    soln_s = s.linear_map(solution)\n",
    "    g_s = s.linear_map(grad)\n",
    "    failing_s = np.zeros(g_s.shape)\n",
    "    failing = np.zeros(grad.shape, np.bool)\n",
    "\n",
    "    # Check the inactive coefficients\n",
    "    failing += s.adjoint_map(np.fabs(g_s) > lagrange * (1 + tol))\n",
    "\n",
    "    # Check the active coefficients\n",
    "    active = soln_s != 0\n",
    "    failing_s[active] += np.fabs(g_s[active] / lagrange + np.sign(soln_s[active])) >= tol \n",
    "    failing += s.adjoint_map(failing_s)\n",
    "    return failing\n",
    "\n",
    "expanded_soln = strong_selector.adjoint_map(sub_soln)\n",
    "full_grad = loss.smooth_objective(expanded_soln, 'grad')\n",
    "failing = check_KKT(full_grad, penalized_selector, expanded_soln, linear_penalty.lagrange)\n",
    "print failing.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have the makings of a full algorithm using the strong rules. Given a candidate\n",
    " active set, we solve the restricted problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(grad_cur, penalized, Xn, Y, soln_cur, lagrange_cur, lagrange_new, active, start_inv_step=1, tol=1.e-6, slope_estimate=2, coef_stop=True, debug=False):\n",
    "    \n",
    "    # try to solve the problem with the active set\n",
    "    active_subproblem, active_selector, pen_active_selector = restricted_problem(Xn, Y, active, lagrange_new)\n",
    "    active_subproblem.coefs[:] = active_selector.linear_map(soln_cur)\n",
    "    active_soln = active_subproblem.solve(start_inv_step=start_inv_step, coef_stop=coef_stop, tol=tol, debug=debug)\n",
    "    soln_cur[:] = active_selector.adjoint_map(active_soln)\n",
    "        \n",
    "    final_inv_step = active_subproblem.final_inv_step\n",
    "    strong = strong_set_lasso(grad_cur, penalized, lagrange_cur, lagrange_new, slope_estimate=slope_estimate)\n",
    "    #strong_subproblem, strong_selector, pen_strong_selector = restricted_problem(Xn, Y, strong, lagrange_new)\n",
    "    #strong_subproblem.coefs[:] = strong_selector.linear_map(soln_cur)\n",
    "    \n",
    "    #grad_cur = strong_selector.adjoint_map(strong_subproblem.smooth_objective(strong_subproblem.coefs, 'grad'))\n",
    "    #soln_cur[:] = strong_selector.adjoint_map(strong_subproblem.coefs)\n",
    "    # are any strong coefficients failing?\n",
    "    #strong_failing = check_KKT(grad_cur,\n",
    "    #                           pen_strong_selector,\n",
    "    #                           soln_cur,\n",
    "    #                           lagrange_new)\n",
    "    #if strong_failing.sum():\n",
    "    #    failing = strong_selector.adjoint_map(strong_failing)\n",
    "    #    return failing + active, final_inv_step, strong\n",
    "    return soln_cur != 0, final_inv_step, strong\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "def main():\n",
    "\n",
    "    lagrange_sequence = lagrange_max * np.exp(np.linspace(np.log(0.05), 0, 100))[::-1]\n",
    "\n",
    "    # scaling will be needed to get coefficients on original scale                                                                                   \\\n",
    "                                                                                                                                                      \n",
    "\n",
    "    scalings = np.asarray(Xn.col_stds).reshape(-1)\n",
    "    final_inv_step = lipschitz / 1000\n",
    "    # first solution corresponding to all zeros except intercept                                                                                     \\\n",
    "                                                                                                                                                      \n",
    "\n",
    "    solution = np.zeros(p)\n",
    "    solution[0] = null_coef\n",
    "\n",
    "    penalized = np.ones(solution.shape, np.bool)\n",
    "    penalized[0] = False\n",
    "    grad = loss.smooth_objective(solution, 'grad')\n",
    "    strong = strong_set_lasso(grad, penalized, lagrange_sequence[0], lagrange_sequence[1])\n",
    "    active = strong.copy()\n",
    "\n",
    "    solutions = [solution.copy()]\n",
    "    rescaled_solutions = [solution.copy()[1:]]\n",
    "    objective = [loss.smooth_objective(solution, 'func')]\n",
    "    dfs = [1]\n",
    "    retry_counter = 0\n",
    "    import time\n",
    "    toc = time.time()\n",
    "    for lagrange_new, lagrange_cur in zip(lagrange_sequence[1:], lagrange_sequence[:-1]):\n",
    "        num_tries = 0\n",
    "        debug = False\n",
    "        tol = 1.0e-5\n",
    "        while True:\n",
    "            active_new, final_inv_step, strong = fit(grad, penalized, Xn,\n",
    "                                             Y, solution, lagrange_cur,\n",
    "                                             lagrange_new, active,\n",
    "                                             tol=tol,\n",
    "                                             start_inv_step=final_inv_step,\n",
    "                                             debug=debug)\n",
    "            grad = loss.smooth_objective(solution, 'grad')\n",
    "            if active_new.sum() <= active.sum() and (~active * active_new).sum() == 0:\n",
    "                failing = check_KKT(grad, penalized, solution, lagrange_new)\n",
    "                if not failing.sum():\n",
    "                    active = (solution != 0) + active\n",
    "                    break\n",
    "                else:\n",
    "                    retry_counter += 1\n",
    "                    print 'trying again:', retry_counter, 'failing:', np.nonzero(failing)[0], active.sum()\n",
    "                    active += strong\n",
    "            else:\n",
    "                print 'active set different', np.nonzero(active), np.nonzero(active_new)\n",
    "                active = active_new + strong \n",
    "                \n",
    "                \n",
    "            tol /= 2.\n",
    "            num_tries += 1\n",
    "            if num_tries % 5 == 0:\n",
    "                debug=True\n",
    "                tol = 1.0e-5\n",
    "                active += strong_set_lasso(grad, penalized, lagrange_cur, lagrange_new)\n",
    "\n",
    "        solutions.append(solution.copy())\n",
    "        rescaled_solutions.append(solution[1:] / scalings[1:])\n",
    "        objective.append(loss.smooth_objective(solution, mode='func'))\n",
    "        dfs.append(active.shape[0])\n",
    "        print lagrange_cur / lagrange_max, lagrange_new, (solution != 0).sum(), 1. - objective[-1] / objective[0], list(lagrange_sequence).index(lagrange_new), np.fabs(rescaled_solutions[-1]).sum()\n",
    "        gc.collect()\n",
    "        tic = time.time()\n",
    "    \n",
    "        print 'time: %0.1f' % (tic-toc)\n",
    "    solutions = scipy.sparse.lil_matrix(solutions)\n",
    "    rescaled_solutions = scipy.sparse.lil_matrix(rescaled_solutions)\n",
    "    objective = np.array(objective)\n",
    "    output = {'devratio': 1 - objective / objective.max(),\n",
    "              'df': dfs,\n",
    "              'beta': solutions,\n",
    "              'lagrange': lagrange_sequence,\n",
    "              'scalings': scalings,\n",
    "              'rescaled_beta': rescaled_solutions}\n",
    "   \n",
    "    scipy.io.savemat('newsgroup_results.mat', output)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally define our problem, by restricting interest to only some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(glmnet)\n",
    "library(Matrix)\n",
    "load(\"NewsGroup.RData\")\n",
    "\n",
    "newsX=NewsGroup$x\n",
    "newsy=NewsGroup$y\n",
    "\n",
    "\n",
    "x=newsX\n",
    "y=(newsy+1)/2\n",
    "n=nrow(newsX)\n",
    "p=ncol(newsX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "%R a=glmnet(x,as.factor(y),stand=TRUE,family=\"binomial\",lambda.min=0.05); print(a$names)\n",
    "\n",
    "tic = time.time()\n",
    "print 'R time: %0.1f ' % (tic-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "newsgroupFit=list(a=a)\n",
    "save(newsgroupFit,file=\"newsgroupFit.RData\")\n",
    "l1norm = apply(abs(a$beta), 2, sum)\n",
    "write.table(data.frame(lagrange=a$lambda, devratio=a$dev.ratio, df=a$df, l1=l1norm), 'newsgroup_output.csv',row.names=FALSE, sep=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
