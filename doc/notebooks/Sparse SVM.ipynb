{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import regreg.api as rr\n",
    "%pylab inline\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss\n",
    "\n",
    "The SVM can be parametrized various ways, one way to write\n",
    "it as a regression problem is to use the hinge loss:\n",
    "$$\n",
    "\\ell(r) = \\max(1-x, 0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hinge = lambda x: np.maximum(1-x, 0)\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "ax = fig.gca()\n",
    "r = np.linspace(-1,2,100)\n",
    "ax.plot(r, hinge(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM loss is then\n",
    "$$\n",
    "\\ell(\\beta) = C \\sum_{i=1}^n h(Y_i X_i^T\\beta) + \\frac{1}{2} \\|\\beta\\|^2_2\n",
    ")\n",
    "$$\n",
    "where $Y_i \\in \\{-1,1\\}$ and $X_i \\in \\mathbb{R}^p$ is one of the feature vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regreg, the hinge loss can be  represented by composition of\n",
    "some of the basic atoms. Specifcally, let $g:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ be the sum of positive part function\n",
    "$$\n",
    "g(z) = \\sum_{i=1}^n\\max(z_i, 0).\n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "\\ell(\\beta) = g\\left(Y \\cdot X\\beta \\right)\n",
    "$$\n",
    "where the product in the parentheses is elementwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_part = np.array([[-1.]])\n",
    "offset = np.array([1.])\n",
    "hinge_rep = rr.positive_part.affine(linear_part, offset, lagrange=1.)\n",
    "hinge_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the loss to be sure it agrees with our original hinge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax.plot(r, [hinge_rep.nonsmooth_objective(v) for v in r])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a vectorized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "P = 200\n",
    "\n",
    "Y = 2 * np.random.binomial(1, 0.5, size=(N,)) - 1.\n",
    "X = np.random.standard_normal((N,P))\n",
    "#X[Y==1] += np.array([30,-20] + (P-2)*[0])[np.newaxis,:]\n",
    "X -= X.mean(0)[np.newaxis, :]\n",
    "hinge_vec = rr.positive_part.affine(-Y[:, None] * X, np.ones_like(Y), lagrange=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = np.ones(X.shape[1])\n",
    "hinge_vec.nonsmooth_objective(beta), np.maximum(1 - Y * X.dot(beta), 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothed hinge\n",
    "\n",
    "For optimization, the hinge loss is not differentiable so it is often\n",
    "smoothed first.\n",
    "\n",
    "The smoothing is applicable to general functions of the form\n",
    "$$\n",
    "g(X\\beta-\\alpha) = g_{\\alpha}(X\\beta)\n",
    "$$\n",
    "where $g_{\\alpha}(z) = g(z-\\alpha)$ \n",
    "and is determined by a small quadratic term\n",
    "$$\n",
    "q(z) = \\frac{C_0}{2} \\|z-x_0\\|^2_2 + v_0^Tz + c_0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.5\n",
    "smoothing_quadratic = rr.identity_quadratic(epsilon, 0, 0, 0)\n",
    "smoothing_quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic terms are determined by four parameters with $(C_0, x_0, v_0, c_0)$.\n",
    "\n",
    "Smoothing of the function by the quadratic $q$ is performed by Moreau smoothing:\n",
    "$$\n",
    "S(g_{\\alpha},q)(\\beta) = \\sup_{z \\in \\mathbb{R}^p} z^T\\beta - g^*_{\\alpha}(z) - q(z)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "g^*_{\\alpha}(z) = \\sup_{\\beta \\in \\mathbb{R}^p} z^T\\beta - g_{\\alpha}(\\beta)\n",
    "$$\n",
    "is the convex (Fenchel) conjugate of the composition $g$ with the translation by\n",
    "$-\\alpha$.\n",
    "\n",
    "The basic atoms in `regreg` know what their conjugate is. Our hinge loss, `hinge_rep`,\n",
    "is the composition of an `atom`, and an affine transform. This affine transform is split\n",
    "into two pieces, the linear part, stored as `linear_transform` and its offset\n",
    "stored as `atom.offset`. It is stored with `atom` as `atom` needs knowledge of\n",
    "this when computing proximal maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hinge_rep.atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hinge_rep.atom.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hinge_rep.linear_transform.linear_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said before, `hinge_rep.atom` knows what its conjugate is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hinge_conj = hinge_rep.atom.conjugate\n",
    "hinge_conj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation $I^{\\infty}$ denotes a constraint. The expression can therefore be parsed as\n",
    "a linear function $\\eta^T\\beta$ plus the function\n",
    "$$\n",
    "g^*(z) = \\begin{cases}\n",
    "0 & 0 \\leq z_i \\leq \\delta \\, \\forall i \\\\\n",
    "\\infty & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The term $\\eta$ is derived from `hinge_rep.atom.offset` and is stored in `hinge_conj.quadratic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hinge_conj.quadratic.linear_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the smoothed hinge loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smoothed_hinge_loss = hinge_rep.smoothed(smoothing_quadratic)\n",
    "smoothed_hinge_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now a smooth function and its objective value and gradient can be computed with\n",
    "`smooth_objective`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax.plot(r, [smoothed_hinge_loss.smooth_objective(v, 'func') for v in r])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "less_smooth = hinge_rep.smoothed(rr.identity_quadratic(5.e-2, 0, 0, 0))\n",
    "ax.plot(r, [less_smooth.smooth_objective(v, 'func') for v in r])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the SVM\n",
    "\n",
    "We can now minimize this objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smoothed_vec = hinge_vec.smoothed(rr.identity_quadratic(0.2, 0, 0, 0))\n",
    "soln = smoothed_vec.solve(tol=1.e-12, min_its=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse SVM\n",
    "\n",
    "We might want to fit a sparse version, adding a sparsifying penalty like the LASSO.\n",
    "This yields the problem\n",
    "$$\n",
    "\\text{minimize}_{\\beta} \\ell(\\beta) + \\lambda \\|\\beta\\|_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "penalty = rr.l1norm(smoothed_vec.shape, lagrange=20)\n",
    "problem = rr.simple_problem(smoothed_vec, penalty)\n",
    "problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparse_soln = problem.solve(tol=1.e-12)\n",
    "sparse_soln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What value of $\\lambda$ should we use? For the $\\ell_1$ penalty in Lagrange form,\n",
    "the smallest $\\lambda$ such that the solution is zero can be found by taking\n",
    "the dual norm, the $\\ell_{\\infty}$ norm, of the gradient of the smooth part at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linf_norm = penalty.conjugate\n",
    "linf_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just computing the conjugate will yield an $\\ell_{\\infty}$ constraint, but this\n",
    "object can still be used to compute the desired value of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_at_zero = smoothed_vec.smooth_objective(np.zeros(smoothed_vec.shape), 'grad')\n",
    "lam_max = linf_norm.seminorm(score_at_zero, lagrange=1.)\n",
    "lam_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "penalty.lagrange = lam_max * 1.001\n",
    "problem.solve(tol=1.e-12, min_its=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "penalty.lagrange = lam_max * 0.99\n",
    "problem.solve(tol=1.e-12, min_its=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path of solutions\n",
    "\n",
    "If we want a path of solutions, we can simply take multiples of `lam_max`. This is similar\n",
    "to the strategy that packages like `glmnet` use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = []\n",
    "lam_vals = (np.linspace(0.05, 1.01, 50) * lam_max)[::-1]\n",
    "for lam_val in lam_vals:\n",
    "    penalty.lagrange = lam_val\n",
    "    path.append(problem.solve(min_its=200).copy())\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.gca()\n",
    "path = np.array(path)\n",
    "ax.plot(path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the penalty\n",
    "\n",
    "We may not want to penalize features the same. We may want some features to be unpenalized.\n",
    "This can be achieved by introducing possibly non-zero feature weights to the $\\ell_1$ norm\n",
    "$$\n",
    "\\beta \\mapsto \\sum_{j=1}^p w_j|\\beta_j|\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = np.random.sample(P) + 1.\n",
    "weights[:5] = 0.\n",
    "weighted_penalty = rr.weighted_l1norm(weights, lagrange=1.)\n",
    "weighted_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_dual = weighted_penalty.conjugate\n",
    "weighted_dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lam_max_weight = weighted_dual.seminorm(score_at_zero, lagrange=1.)\n",
    "lam_max_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_problem = rr.simple_problem(smoothed_vec, weighted_penalty)\n",
    "path = []\n",
    "lam_vals = (np.linspace(0.05, 1.01, 50) * lam_max_weight)[::-1]\n",
    "for lam_val in lam_vals:\n",
    "    weighted_penalty.lagrange = lam_val\n",
    "    path.append(weighted_problem.solve(min_its=200).copy())\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.gca()\n",
    "path = np.array(path)\n",
    "ax.plot(path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 5 coefficients that are not penalized hence they are nonzero the entire path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group LASSO\n",
    "\n",
    "Variables may come in groups. A common penalty for this setting is the group LASSO.\n",
    "Let $$\n",
    "\\{1, \\dots, p\\} = \\cup_{g \\in G} g\n",
    "$$\n",
    "be a partition of the set of features and $w_g$ a weight for each group. The \n",
    "group LASSO penalty is\n",
    "$$\n",
    "\\beta \\mapsto \\sum_{g \\in G} w_g \\|\\beta_g\\|_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = []\n",
    "for i in range(P/5):\n",
    "    groups.extend([i]*5)\n",
    "weights = dict([g, np.random.sample()+1] for g in np.unique(groups))\n",
    "group_penalty = rr.group_lasso(groups, weights=weights, lagrange=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_dual = group_penalty.conjugate\n",
    "lam_max_group = group_dual.seminorm(score_at_zero, lagrange=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_problem = rr.simple_problem(smoothed_vec, group_penalty)\n",
    "path = []\n",
    "lam_vals = (np.linspace(0.05, 1.01, 50) * lam_max_group)[::-1]\n",
    "for lam_val in lam_vals:\n",
    "    group_penalty.lagrange = lam_val\n",
    "    path.append(group_problem.solve(min_its=200).copy())\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.gca()\n",
    "path = np.array(path)\n",
    "ax.plot(path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, variables enter in groups here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bound form\n",
    "\n",
    "The common norm atoms also have a bound form. That is, we can just as easily solve the \n",
    "problem\n",
    "$$\n",
    "\\text{minimize}_{\\beta: \\|\\beta\\|_1 \\leq \\delta}\\ell(\\beta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bound_l1 = rr.l1norm(P, bound=2.)\n",
    "bound_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bound_problem = rr.simple_problem(smoothed_vec, bound_l1)\n",
    "bound_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bound_soln = bound_problem.solve()\n",
    "np.fabs(bound_soln).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
