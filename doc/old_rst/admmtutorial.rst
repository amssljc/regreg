.. _admmtutorial:

ADMM tutorial
~~~~~~~~~~~~~

This tutorial illustrates solving the :ref:`fusedlassoapprox` problem with the ADMM algorithm.

The sparse fused lasso minimizes the objective

.. math::

   \frac{1}{2}||y - \beta||^{2}_{2} + \lambda_{1}||D\beta||_{1} + \lambda_2 \|\beta\|_1

with

.. math::

      D = \left(\begin{array}{rrrrrr} -1 & 1 & 0 & 0 & \cdots & 0 \\ 0 & -1 & 1 & 0 & \cdots & 0 \\ &&&&\cdots &\\ 0 &0&0&\cdots & -1 & 1 \end{array}\right)

The default ADMM algorithm in RegReg (described in :ref:`algorithms`) solves this problem by making the substitutions :math:`z_1 = \beta` and :math:`z_2 = D\beta` and cyclically minimizing the augmented Lagrangian

    .. math::

       \frac{1}{2}||y - \beta||^{2}_{2}  + \sum_i \lambda_i \|z_i\|_1 + \sum_i u_i^T(z_i - D_i \beta) + \frac{\rho}{2} \sum_i \|z_i - D_i\beta\|_2^2 

over the variables :math:`\beta`, :math:`z_i` and :math:`u_i`.

To solve this problem using  RegReg we begin by loading the necessary numerical libraries

.. ipython2::

   import numpy as np
   import pylab	
   from scipy import sparse

and the RegReg classes necessary for this problem,

.. ipython::

   import regreg.api as R

Next, let's generate an example signal,

.. ipython::
 
   Y = np.random.standard_normal(500); Y[100:150] += 7; Y[250:300] += 14

which looks like

.. plot::

   import numpy as np
   import pylab
   Y = np.random.standard_normal(500); Y[100:150] += 7; Y[250:300] += 14
   pylab.scatter(np.arange(Y.shape[0]), Y)

Now we can create the problem object, beginning with the loss function

.. ipython::

   loss = R.signal_approximator(Y)

there are other loss functions (squared error, logistic, etc) and any differentiable function can be specified. Next, we specifiy the seminorm for this problem by instantiating two l1norm objects,

.. ipython::

   sparsity = R.l1norm(len(Y), lagrange=0.8)

which creates an l1norm object with :math:`\lambda_2=0.8`. The first argument specifies the length of the coefficient vector. The object sparsity now has a coefficient associated with it that we can access and change,

.. ipython::

   sparsity.lagrange
   sparsity.lagrange += 1
   sparsity.lagrange

Next, we create the fused lasso matrix and the associated l1norm object,

.. ipython::

   D = (np.identity(500) + np.diag([-1]*499,k=1))[:-1]
   D
   D = sparse.csr_matrix(D)
   fused = R.l1norm.linear(D, lagrange=25.5)

Here we first created D, converted it a sparse matrix, and then created an l1norm object with the sparse version of D and :math:`\lambda_1 = 25.5`. We can now combine the two l1norm objects and the loss function using the  container class

.. ipython::

   problem = R.container(loss, sparsity, fused)

.. ipython::

   solver = R.admm_problem(problem)
   solver.fit(max_its=1000, tol=1e-8)
   solution = solver.beta

We can then plot solution to see the result of the regression,

.. plot:: 

    from scipy import sparse

    import regreg.api as R
    Y = np.random.standard_normal(500); Y[100:150] += 7; Y[250:300] += 14
    loss = R.signal_approximator(Y)
    sparsity = R.l1norm(len(Y), lagrange=0.8)
    D = (np.identity(500) + np.diag([-1]*499,k=1))[:-1]
    D = sparse.csr_matrix(D)
    fused = R.l1norm.linear(D, lagrange=25.5)
    problem = R.container(loss, sparsity, fused)
    solver = R.admm_problem(problem)
    solver.fit(max_its=1000, tol=1e-8)
    solution = solver.beta
    pylab.plot(solution, c='g', linewidth=3)	
    pylab.scatter(np.arange(Y.shape[0]), Y)
